{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:10.195498Z","iopub.execute_input":"2024-12-04T07:01:10.195947Z","iopub.status.idle":"2024-12-04T07:01:10.656074Z","shell.execute_reply.started":"2024-12-04T07:01:10.195907Z","shell.execute_reply":"2024-12-04T07:01:10.655152Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install git+https://github.com/Farama-Foundation/MAgent2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:10.658144Z","iopub.execute_input":"2024-12-04T07:01:10.658747Z","iopub.status.idle":"2024-12-04T07:01:53.043378Z","shell.execute_reply.started":"2024-12-04T07:01:10.658698Z","shell.execute_reply":"2024-12-04T07:01:53.041937Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Farama-Foundation/MAgent2\n  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-hwe7j8yk\n  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-hwe7j8yk\n  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.26.4)\nCollecting pygame>=2.1.0 (from magent2==0.3.3)\n  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: pettingzoo>=1.23.1 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.24.0)\nRequirement already satisfied: gymnasium>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (0.29.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\nDownloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: magent2\n  Building wheel for magent2 (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1657662 sha256=65241249cdd16cf5d4ec753fca81ba80bc5d44832b0c1e62b232d0eb605d5a79\n  Stored in directory: /tmp/pip-ephem-wheel-cache-p6pw5vi3/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\nSuccessfully built magent2\nInstalling collected packages: pygame, magent2\nSuccessfully installed magent2-0.3.3 pygame-2.6.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\nclass ConvBNAvgPool(nn.Module):\n    def __init__(self, in_channels, outs_channels, conv):\n        \"\"\"\n        Args:\n            in_channels: int\n            outs_channels: List\n            conv:\n                2 for conv2d\n                3 for conv3d\n        \"\"\"\n        super(ConvBNAvgPool, self).__init__()\n\n        if conv == 3:\n            Conv = nn.Conv3d\n            BatchNorm = nn.BatchNorm3d\n            avg_pool = nn.AdaptiveAvgPool3d\n            size_avg_pool = (1,1,1)\n        else:\n            Conv = nn.Conv2d\n            BatchNorm = nn.BatchNorm2d\n            avg_pool = nn.AdaptiveAvgPool2d\n            size_avg_pool = (1,1)\n\n        channels = [in_channels] + outs_channels\n        \n        self.model = nn.Sequential(\n            *[nn.Sequential(\n                Conv(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                BatchNorm(out_channels),\n                nn.ReLU()\n            ) for in_channels, out_channels in zip(channels[:-1], channels[1:])]\n        )\n        \n        # Lớp Average Pooling\n        self.avgpool = avg_pool(size_avg_pool)  # Chuyển kích thước về (1, 1, 1)\n\n    def forward(self, x):\n        # Áp dụng Conv3D, BatchNorm và ReLU cho mỗi lớp\n        x = self.model(x)\n        \n        # Áp dụng Avg Pooling để chuẩn hóa về (batch_size, out_channels)\n        x = self.avgpool(x)\n        \n        # Lấy output có kích thước (batch_size, out_channels)\n        x = x.view(x.size(0), -1)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:53.045268Z","iopub.execute_input":"2024-12-04T07:01:53.045678Z","iopub.status.idle":"2024-12-04T07:01:53.057993Z","shell.execute_reply.started":"2024-12-04T07:01:53.045640Z","shell.execute_reply":"2024-12-04T07:01:53.056560Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class Mlp(nn.Module):\n    def __init__(self, in_features, outs_features):\n        \"\"\"\n        Args:\n            in_channels: int\n            outs_channels: List\n        \"\"\"\n        super().__init__()\n\n        channels = [in_features] + outs_features\n\n        self.mlp = nn.Sequential(\n            *[nn.Sequential(\n                nn.Linear(in_channels, out_channels),\n                nn.ReLU()\n            ) for in_channels, out_channels in zip(channels[:-1], channels[1:])]\n        )\n\n    def forward(self, x):\n        x = self.mlp(x)\n\n        return x\n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:53.060503Z","iopub.execute_input":"2024-12-04T07:01:53.060889Z","iopub.status.idle":"2024-12-04T07:01:53.087286Z","shell.execute_reply.started":"2024-12-04T07:01:53.060817Z","shell.execute_reply":"2024-12-04T07:01:53.086120Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Critic(nn.Module):\n    \"\"\"\n    Args:\n        obs: Bx5xNx13x13\n        action: BxN\n        \n    Return:\n        Value: Bx1\n    \"\"\"\n    \n    def __init__(self, in_channels_obs, outs_channels_obs, in_channels_act, num_layer_mlp):\n        super().__init__()\n\n        conv = 3\n        self.model_obs = ConvBNAvgPool(in_channels_obs, outs_channels_obs, conv) #Bxout_channels\n\n        out_channels_act = outs_channels_obs[-1]\n        self.model_act = nn.Linear(in_channels_act, out_channels_act)\n\n        in_channels_mlp = out_channels_act*2\n        outs_channels_mlp = [in_channels_mlp*2]*num_layer_mlp + [1]\n        \n        self.mlp = Mlp(in_channels_mlp, outs_channels_mlp)\n        \n    def forward(self, obs, act):\n        obs = self.model_obs(obs)\n        act = self.model_act(act)\n        # concat obs, act\n        out = torch.cat((obs, act), dim=-1)\n        out = self.mlp(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:53.088667Z","iopub.execute_input":"2024-12-04T07:01:53.089075Z","iopub.status.idle":"2024-12-04T07:01:53.105468Z","shell.execute_reply.started":"2024-12-04T07:01:53.089038Z","shell.execute_reply":"2024-12-04T07:01:53.104261Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Actor(nn.Module):\n    \"\"\"\n    Agent chỉ quan sát môi trường xung quanh nó để đưa ra hành động\n    Args:\n        obs: Bx5x13x13\n        \n    Return:\n        Value: Bxlen_action_space\n    \"\"\"\n    def __init__(self, in_channels, outs_channels, len_action_space, num_layer_mlp):\n        super().__init__()\n\n        in_channels_mlp = outs_channels[-1]\n        outs_channels_mlp = [in_channels_mlp*2]*num_layer_mlp + [len_action_space]\n        \n        mlp = Mlp(in_channels_mlp, outs_channels_mlp)\n        \n        conv = 2\n        self.model = nn.Sequential(\n            ConvBNAvgPool(in_channels, outs_channels, conv),\n            mlp\n        )\n    def forward(self, x):\n        x = self.model(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:53.107168Z","iopub.execute_input":"2024-12-04T07:01:53.107630Z","iopub.status.idle":"2024-12-04T07:01:53.119441Z","shell.execute_reply.started":"2024-12-04T07:01:53.107578Z","shell.execute_reply":"2024-12-04T07:01:53.118397Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from magent2.environments import battle_v4\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nNUM_EPISODES = 100\nMAX_EPISODE_LENGTH = 100\nMAX_CYCLES = MAX_EPISODE_LENGTH * 3\nMAP_SIZE = 45\nNUM_AGENTS = 81\nLEN_ACTION_SPACE = 21\nCAPACITY = 1000\nTRAIN_AGENT = 'red'\n\nenv = battle_v4.env(map_size=MAP_SIZE, minimap_mode=False, extra_features=False, render_mode='rgb_array', max_cycles=MAX_CYCLES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:53.120761Z","iopub.execute_input":"2024-12-04T07:01:53.121908Z","iopub.status.idle":"2024-12-04T07:01:53.733551Z","shell.execute_reply.started":"2024-12-04T07:01:53.121857Z","shell.execute_reply":"2024-12-04T07:01:53.732546Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import numpy as np\n\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.buffer = {'obs': [], 'act': [], 'reward': [], 'obs1': [], 'act1': []}\n        self.size = 0\n\n    def push(self, obs, act, reward, obs1, act1):\n        if self.size < self.capacity:\n            for key, value in zip(self.buffer.keys(), [obs, act, reward, obs1, act1]):\n                self.buffer[key].append(value)\n            self.size += 1\n        else:\n            for key, value in zip(self.buffer.keys(), [obs, act, reward, obs1, act1]):\n                self.buffer[key][self.size % self.capacity] = value\n\n    def sample(self, batch_size):\n        indices = np.random.choice(self.size, batch_size, replace=False)\n        return {key: np.array([self.buffer[key][i] for i in indices]) for key in self.buffer.keys()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:53.735052Z","iopub.execute_input":"2024-12-04T07:01:53.735529Z","iopub.status.idle":"2024-12-04T07:01:53.746047Z","shell.execute_reply.started":"2024-12-04T07:01:53.735473Z","shell.execute_reply":"2024-12-04T07:01:53.744851Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\n@torch.no_grad()\ndef update_target_model(model_online, model_target, tau):\n    for online, target in zip(model_online.parameters(), model_target.parameters()):\n        target.data.lerp_(online.data, tau)\n\ndef train(\n    model_critics: list, \n    model_actors: list,\n    model_target_critics: list,\n    model_target_actors: list,\n    num_agents: int,\n    replay_buffer: ReplayBuffer,\n    batch_size: int,\n    gamma: float,\n    tau: float,\n    optimizer_critics,\n    optimizer_actors,\n    device\n):\n    mse_loss = nn.MSELoss()\n    for agent_i in range(num_agents):\n        # Đưa mô hình vào trạng thái train\n        model_actors[agent_i].train()\n        model_critics[agent_i].train()\n        \n        batch = replay_buffer.sample(batch_size)\n        \"\"\"\n            Chuyển x, a, r, x' về dạng tensor, kích thước của từng loại:\n            x: Bx5xNx13x13\n            a: BxN\n            r: BxN\n            x': Bx5xNx13x13\n        \"\"\"\n        x_j = torch.tensor(batch['obs'], dtype=torch.float32, device=device).permute(0, 4, 1, 2, 3)\n        a_j = torch.tensor(batch['act'], dtype=torch.float32, device=device)\n        r_j = torch.tensor(batch['reward'], dtype=torch.float32, device=device)\n        x1_j = torch.tensor(batch['obs1'], dtype=torch.float32, device=device).permute(0, 4, 1, 2, 3)\n        a1_j = torch.tensor(batch['act1'], dtype=torch.float32, device=device)\n\n        with torch.no_grad():    \n            y_j = r_j[:,agent_i] + gamma * model_target_critics[agent_i](x1_j, a1_j).squeeze()\n\n        optimizer_critics[agent_i].zero_grad()\n        L = mse_loss(y_j, model_critics[agent_i](x_j, a_j).squeeze())\n\n        L.backward()\n        optimizer_critics[agent_i].step()\n\n        grad_a_i = []\n        for idx_batch in range(batch_size):\n            optimizer_critics[agent_i].zero_grad()\n\n            a_j_idx = a_j[idx_batch].unsqueeze(0).requires_grad_()\n            x_j_idx = x_j[idx_batch].unsqueeze(0).detach()\n\n            Q = model_critics[agent_i](x_j_idx, a_j_idx)\n\n            grad_a_i.append(\n                torch.autograd.grad(\n                    Q, a_j_idx\n                )[0].squeeze()[agent_i]\n            )\n\n        grad_a_i = torch.stack(grad_a_i).unsqueeze(-1).detach()\n\n        optimizer_actors[agent_i].zero_grad()\n\n        x_j_agent_i = x_j[:, :, agent_i, :, :]\n\n        J = -(grad_a_i[:, agent_i] * model_actors[agent_i](x_j_agent_i).max(dim=-1)[0]).mean()\n\n        J.backward()\n\n        optimizer_actors[agent_i].step()\n\n    \n    # Cập nhật tham số mạng mục tiêu với mỗi agent i\n    for agent_i in range(num_agents):\n        update_target_model(model_critics[agent_i], model_target_critics[agent_i], tau)\n        update_target_model(model_actors[agent_i], model_target_actors[agent_i], tau)\n        \n    print(\"completed!\")\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:53.747788Z","iopub.execute_input":"2024-12-04T07:01:53.748198Z","iopub.status.idle":"2024-12-04T07:01:53.764138Z","shell.execute_reply.started":"2024-12-04T07:01:53.748161Z","shell.execute_reply":"2024-12-04T07:01:53.762765Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import numpy as np\ndef setup_config():\n    num_agents = NUM_AGENTS\n    len_action_space = LEN_ACTION_SPACE\n    num_layer_mlp = 1\n\n    model_critics = [Critic(5, [32], num_agents, num_layer_mlp).to(device) for i in range(num_agents)]\n    model_actors = [Actor(5, [32], len_action_space, num_layer_mlp).to(device) for i in range(num_agents)]\n\n    model_target_critics = [\n        Critic(5, [32], num_agents, num_layer_mlp).to(device) for model in model_critics\n    ]\n\n    model_target_actors = [\n        Actor(5, [32], len_action_space, num_layer_mlp).to(device) for model in model_actors\n    ]\n\n    # Sao chép trọng số từ online models sang target models\n    for target, source in zip(model_target_critics, model_critics):\n        target.load_state_dict(source.state_dict())\n    \n    for target, source in zip(model_target_actors, model_actors):\n        target.load_state_dict(source.state_dict())\n\n    replay_buffer = ReplayBuffer(CAPACITY)\n\n    batch_size = 32\n\n    gamma = 0.9\n\n    tau = 1e-3\n\n    lr = 1e-4\n    optimizer_critics = [optim.Adam(model.parameters(), lr) for model in model_critics]\n    optimizer_actors = [optim.Adam(model.parameters(), lr) for model in model_actors]\n\n    device = DEVICE\n    \n    config = {\n        'num_agents': num_agents,\n        'model_critics': model_critics, \n        'model_actors': model_actors,\n        'model_target_critics': model_target_critics,\n        'model_target_actors': model_target_actors,\n        'replay_buffer': replay_buffer,\n        'batch_size': batch_size,\n        'gamma': gamma,\n        'tau': tau,\n        'optimizer_critics': optimizer_critics,\n        'optimizer_actors': optimizer_actors,\n        'device': device\n    }\n    return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:01:53.767264Z","iopub.execute_input":"2024-12-04T07:01:53.767688Z","iopub.status.idle":"2024-12-04T07:01:53.780071Z","shell.execute_reply.started":"2024-12-04T07:01:53.767638Z","shell.execute_reply":"2024-12-04T07:01:53.778968Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import numpy as np\ndef test_train():\n    config = setup_config()\n    replay_buffer = config['replay_buffer']\n    for i in range(capacity):\n        dummy_x = [np.random.rand(13,13,5) for i in range(num_agents)]\n        dummy_a = [np.random.rand() for i in range(num_agents)]\n        dummy_r = [np.random.rand() for i in range(num_agents)]\n        dummy_x1 = [np.random.rand(13,13,5) for i in range(num_agents)]\n        dummy_a1 = [np.random.rand() for i in range(num_agents)]\n\n        replay_buffer.push(dummy_x, dummy_a, dummy_r, dummy_x1, dummy_a1)\n    train(**config)\n# test_train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:22:11.092764Z","iopub.execute_input":"2024-12-04T06:22:11.093235Z","iopub.status.idle":"2024-12-04T06:22:20.309640Z","shell.execute_reply.started":"2024-12-04T06:22:11.093209Z","shell.execute_reply":"2024-12-04T06:22:20.308648Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 81/81 [00:07<00:00, 11.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"completed!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nclass Transition:\n    def __init__(self, num_agents):\n        self.num_agents = num_agents\n        \n        self.obs = self._setup_agents_data()\n        self.act = self._setup_agents_data()\n        self.reward = self._setup_agents_data()\n        self.obs1 = self._setup_agents_data()\n        self.act1 = self._setup_agents_data()\n        \n    def _setup_agents_data(self):\n        return {\n            'red': {f'red_{i}': None for i in range(self.num_agents)},\n            'blue': {f'blue_{i}': None for i in range(self.num_agents)}\n        }\n\n    def reset(self):\n        \"\"\"Reset all attributes to their initial empty state.\"\"\"\n        self.obs = self._setup_agents_data()\n        self.act = self._setup_agents_data()\n        self.reward = self._setup_agents_data()\n        self.obs1 = self._setup_agents_data()\n        self.act1 = self._setup_agents_data()\n\n    def is_all_none(self):\n        \"\"\"Kiểm tra nếu tất cả các giá trị trong obs, act, reward, obs1 là None.\"\"\"\n        def check_none(data):\n            return all(\n                value is None \n                for group in data.values() \n                for value in group.values()\n            )\n        \n        return (\n            check_none(self.obs) and \n            check_none(self.act) and \n            check_none(self.reward) and \n            check_none(self.obs1) and\n            check_none(self.act1)\n        )\n        \n    def _padding(self, lst):\n        dummy = [element for element in lst if element is not None][0]\n\n        if isinstance(dummy, np.ndarray):\n            shape = dummy.shape\n            lst = [np.zeros(shape) if element is None else element for element in lst]\n            \n        else:\n            lst = [0 if element is None else element for element in lst]\n            \n        return lst\n        \n    def get_transition(self, handle):\n        obs = self._padding(list(self.obs[handle].values()))\n        act = self._padding(list(self.act[handle].values()))\n        reward = self._padding(list(self.reward[handle].values()))\n        obs1 = self._padding(list(self.obs1[handle].values()))\n        act1 = self._padding(list(self.act1[handle].values()))\n        return (obs, act, reward, obs1, act1)\n\ntransition = Transition(NUM_AGENTS)\n\nconfig = setup_config()\n\nreplay = config['replay_buffer']\n\nis_step = {\n    1: False,\n    2: False,\n    3: True\n}\ndef rotate_steps(is_step):\n    # Kiểm tra xem có nhiều hơn một giá trị True không\n    if sum(is_step.values()) > 1:\n        raise ValueError(\"is_step contains more than one True value\")\n        \n    # Tìm bước hiện tại có giá trị True\n    current_step = next((key for key, value in is_step.items() if value), None)\n    \n    if current_step is None:\n        raise ValueError(\"No active step found in is_step\")\n\n    # Đặt giá trị False cho bước hiện tại\n    is_step[current_step] = False\n    \n    # Tìm bước tiếp theo, hoặc quay lại bước đầu tiên nếu đang ở bước cuối\n    next_step = current_step + 1 if current_step + 1 in is_step else min(is_step.keys())\n    \n    # Đặt giá trị True cho bước tiếp theo\n    is_step[next_step] = True\n\n    return is_step\n    \ncurrent_handle = None\ncurrent_cycle = -1 #0\n\nreward_episodes = []\n#x,a,r,x',a', 1 episode\nfor episode in tqdm(range(NUM_EPISODES)):\n    rewards = []\n    env.reset()\n    for agent in env.agent_iter():\n        agent_handle, agent_indice = agent.split(\"_\")\n        observation, reward, termination, truncation, info = env.last()\n        \n        if agent_handle != current_handle:\n            current_handle = agent_handle\n            current_cycle += 1\n            \n            if current_cycle % 2 == 0:\n                # Đảm bảo rằng chuyển step sau mỗi cycle và cycle đầu tiên step = 1\n                is_step = rotate_steps(is_step)\n    \n                if current_cycle % 3 == 0:\n                    # Đảm bảo rằng lưu trữ lại transition vào replay\n                    # Cuối bước Sampling\n                    if not transition.is_all_none():\n                        replay.push(*transition.get_transition(TRAIN_AGENT))\n                        \n                    # Training\n                    # Code train here\n                    train(**config)\n                    \n                    # Đảm bảo rằng transition reset sau mỗi 3 bước\n                    transition.reset()\n                \n            # current_handle = agent_handle\n            # current_cycle += 1\n            \n        # bước 1: lấy tất cả các observation của đội xanh và đỏ (trong cycle 1)\n        if is_step[1]:\n            transition.obs[agent_handle][agent] = observation\n            action = None if termination or truncation else 0\n            env.step(action)\n            \n        # bước 2: Thực hiện từng hành động của agent (trong cycle 2)\n        if is_step[2]:\n            if termination or truncation:\n                action = None\n                \n            else:\n                if agent_handle == 'red':\n                    # Thêm hành động khác tại đây, mặc định là random\n                    # action = env.action_space(agent).sample()\n                    obs = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(dim=-1)\n                    obs = obs.permute(0,3,1,2)\n                    action = config['model_actors'][int(agent_indice)](obs).max(dim=-1)[0].squeeze().item()\n                    \n                else:\n                    # Thêm hành động khác tại đây, mặc định là random\n                    action = env.action_space(agent).sample()\n                    \n            transition.act[agent_handle][agent] = action\n            env.step(action)\n            \n        # bước 3: Lấy phần thưởng và quan sát hiện tại của từng agent (trong cycle 3)\n        if is_step[3]:\n            transition.reward[agent_handle][agent] = reward\n            transition.obs1[agent_handle][agent] = observation\n            if agent_handle == 'red':\n                # Thêm hành động khác tại đây, mặc định là random (Sử dụng model target)\n                act1 = env.action_space(agent).sample()\n                rewards.append(reward)\n                \n            else:\n                # Thêm hành động khác tại đây, mặc định là random (Sử dụng model target)\n                act1 = env.action_space(agent).sample()\n\n            transition.act1[agent_handle][agent] = act1\n            \n            action = None if termination or truncation else 0\n            env.step(action)\n        \nprint(env.agent_selection)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:40:17.837638Z","iopub.execute_input":"2024-12-04T03:40:17.838099Z","iopub.status.idle":"2024-12-04T04:12:02.268113Z","shell.execute_reply.started":"2024-12-04T03:40:17.838063Z","shell.execute_reply":"2024-12-04T04:12:02.266187Z"}},"outputs":[{"name":"stderr","text":"  4%|▍         | 435/10000 [31:44<11:37:49,  4.38s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39magent_iter():\n\u001b[1;32m    113\u001b[0m     agent_handle \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 114\u001b[0m     observation, reward, termination, truncation, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent_handle \u001b[38;5;241m!=\u001b[39m current_handle:\n\u001b[1;32m    117\u001b[0m         current_handle \u001b[38;5;241m=\u001b[39m agent_handle\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"replay.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T11:33:49.699420Z","iopub.execute_input":"2024-12-03T11:33:49.699837Z","iopub.status.idle":"2024-12-03T11:33:49.708919Z","shell.execute_reply.started":"2024-12-03T11:33:49.699803Z","shell.execute_reply":"2024-12-03T11:33:49.707225Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"400"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef sample_gumbel(shape, eps=1e-20, device='cpu'):\n    \"\"\"\n    Lấy mẫu nhiễu Gumbel từ phân phối Gumbel(0, 1).\n    \"\"\"\n    U = torch.rand(shape, device=device)\n    return -torch.log(-torch.log(U + eps) + eps)\n\ndef gumbel_softmax(logits, tau=1.0, hard=False):\n    \"\"\"\n    Thực hiện Gumbel Softmax.\n    \n    Args:\n        logits (torch.Tensor): Input tensor, là các logit trước khi qua softmax.\n        tau (float): Tham số nhiệt độ.\n        hard (bool): Nếu True, lấy mẫu one-hot, ngược lại trả về phân phối xác suất.\n        \n    Returns:\n        torch.Tensor: Xác suất (hoặc mẫu one-hot nếu `hard=True`).\n    \"\"\"\n    # Lấy mẫu nhiễu Gumbel\n    gumbel_noise = sample_gumbel(logits.shape, device=logits.device)\n    # Tính softmax với nhiễu Gumbel\n    y = F.softmax((logits + gumbel_noise) / tau, dim=-1)\n\n    if hard:\n        # Lấy mẫu one-hot (gần giống argmax)\n        y_hard = torch.zeros_like(y)\n        y_hard.scatter_(-1, y.argmax(dim=-1, keepdim=True), 1.0)\n        # Kết hợp giá trị gradient từ softmax\n        y = (y_hard - y).detach() + y\n    return y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:48:51.030434Z","iopub.execute_input":"2024-12-04T09:48:51.030889Z","iopub.status.idle":"2024-12-04T09:48:51.040905Z","shell.execute_reply.started":"2024-12-04T09:48:51.030850Z","shell.execute_reply":"2024-12-04T09:48:51.039351Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"logits = torch.tensor([[2.0, 2.1, 2.2]])\n\n# Gumbel Softmax với nhiệt độ khác nhau\ny_soft = gumbel_softmax(logits, tau=0.01, hard=False)\ny_hard = gumbel_softmax(logits, tau=0.01, hard=True)\n\nprint(f\"Logits: {logits}\")\nprint(f\"Soft Gumbel Softmax: {y_soft}\")\nprint(f\"Hard Gumbel Softmax (One-Hot): {y_hard}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:19:06.447355Z","iopub.execute_input":"2024-12-04T10:19:06.447758Z","iopub.status.idle":"2024-12-04T10:19:06.457311Z","shell.execute_reply.started":"2024-12-04T10:19:06.447723Z","shell.execute_reply":"2024-12-04T10:19:06.456121Z"}},"outputs":[{"name":"stdout","text":"Logits: tensor([[2.0000, 2.1000, 2.2000]])\nSoft Gumbel Softmax: tensor([[8.2677e-44, 0.0000e+00, 1.0000e+00]])\nHard Gumbel Softmax (One-Hot): tensor([[0., 1., 0.]])\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}