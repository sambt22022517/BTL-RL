{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e200f98",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T05:06:56.795464Z",
     "iopub.status.busy": "2024-12-15T05:06:56.795131Z",
     "iopub.status.idle": "2024-12-15T05:07:32.219748Z",
     "shell.execute_reply": "2024-12-15T05:07:32.218841Z"
    },
    "papermill": {
     "duration": 35.431682,
     "end_time": "2024-12-15T05:07:32.221929",
     "exception": false,
     "start_time": "2024-12-15T05:06:56.790247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Farama-Foundation/MAgent2\r\n",
      "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-2i38s54c\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-2i38s54c\r\n",
      "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.26.4)\r\n",
      "Collecting pygame>=2.1.0 (from magent2==0.3.3)\r\n",
      "  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: pettingzoo>=1.23.1 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.24.0)\r\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (0.29.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\r\n",
      "Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: magent2\r\n",
      "  Building wheel for magent2 (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696137 sha256=42cad2369f961d5a7edc538bc22523dd5b053e33db49084576b005f805d9be46\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-150l3vdb/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\r\n",
      "Successfully built magent2\r\n",
      "Installing collected packages: pygame, magent2\r\n",
      "Successfully installed magent2-0.3.3 pygame-2.6.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/Farama-Foundation/MAgent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f61a49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:07:32.234836Z",
     "iopub.status.busy": "2024-12-15T05:07:32.234508Z",
     "iopub.status.idle": "2024-12-15T05:07:35.236165Z",
     "shell.execute_reply": "2024-12-15T05:07:35.235054Z"
    },
    "papermill": {
     "duration": 3.010292,
     "end_time": "2024-12-15T05:07:35.238165",
     "exception": false,
     "start_time": "2024-12-15T05:07:32.227873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RL-final-project-AIT-3007'...\r\n",
      "remote: Enumerating objects: 56, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (24/24), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\r\n",
      "remote: Total 56 (delta 12), reused 13 (delta 6), pack-reused 32 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (56/56), 17.78 MiB | 20.20 MiB/s, done.\r\n",
      "Resolving deltas: 100% (23/23), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/giangbang/RL-final-project-AIT-3007.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508a2031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:07:35.253411Z",
     "iopub.status.busy": "2024-12-15T05:07:35.252648Z",
     "iopub.status.idle": "2024-12-15T05:07:38.406364Z",
     "shell.execute_reply": "2024-12-15T05:07:38.405640Z"
    },
    "papermill": {
     "duration": 3.164137,
     "end_time": "2024-12-15T05:07:38.408336",
     "exception": false,
     "start_time": "2024-12-15T05:07:35.244199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/model-qnet')\n",
    "from model_deep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d2c8bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:07:38.421837Z",
     "iopub.status.busy": "2024-12-15T05:07:38.421473Z",
     "iopub.status.idle": "2024-12-15T05:07:38.425242Z",
     "shell.execute_reply": "2024-12-15T05:07:38.424585Z"
    },
    "papermill": {
     "duration": 0.012326,
     "end_time": "2024-12-15T05:07:38.426707",
     "exception": false,
     "start_time": "2024-12-15T05:07:38.414381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/RL-final-project-AIT-3007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f85b0255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:07:38.438961Z",
     "iopub.status.busy": "2024-12-15T05:07:38.438709Z",
     "iopub.status.idle": "2024-12-15T05:07:38.897545Z",
     "shell.execute_reply": "2024-12-15T05:07:38.896865Z"
    },
    "papermill": {
     "duration": 0.467319,
     "end_time": "2024-12-15T05:07:38.899614",
     "exception": false,
     "start_time": "2024-12-15T05:07:38.432295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, Counter\n",
    "import os\n",
    "from magent2.environments import battle_v4\n",
    "import time\n",
    "# from torch_model import QNetwork\n",
    "\n",
    "class MyQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], 13, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(13, 13, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(13, 13, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(512, 256),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(256, 128),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(128, action_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        out = self.cnn(x)\n",
    "\n",
    "        # print(out.shape)\n",
    "        \n",
    "        # out += self.skip_connection(x)\n",
    "\n",
    "        # out = self.cnn2(out)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "            \n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "            \n",
    "        out = out.reshape(batchsize, -1)\n",
    "        \n",
    "        return self.fc(out)\n",
    "\n",
    "class ReplayBuffer(Dataset):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.buffer[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d542ed3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:07:38.913142Z",
     "iopub.status.busy": "2024-12-15T05:07:38.912854Z",
     "iopub.status.idle": "2024-12-15T05:07:38.931781Z",
     "shell.execute_reply": "2024-12-15T05:07:38.931213Z"
    },
    "papermill": {
     "duration": 0.027546,
     "end_time": "2024-12-15T05:07:38.933247",
     "exception": false,
     "start_time": "2024-12-15T05:07:38.905701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "class Trainer:\n",
    "    def __init__(self, env, config_qnet=None, input_shape=None, action_shape=None, learning_rate=1e-3):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_network = MyQNetwork(input_shape, action_shape).to(self.device)\n",
    "        # self.q_network = QNet(config_qnet, device=self.device)\n",
    "        # self.q_network.load_state_dict(\n",
    "        #     torch.load(\"/kaggle/input/blue-improved/blue_improved.pt\", weights_only=True)\n",
    "        # )\n",
    "        # self.target_network = QNet(config_qnet, device=self.device)\n",
    "        self.target_network = MyQNetwork(input_shape, action_shape).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        # self.red_pretrained_network = QNetwork(input_shape, action_shape).to(self.device)\n",
    "        # self.red_pretrained_network.load_state_dict(\n",
    "        #     torch.load(\"/kaggle/working/RL-final-project-AIT-3007/red.pt\", weights_only=True)\n",
    "        # )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.steplr = lr_scheduler.StepLR(optimizer=self.optimizer, step_size=1, gamma=0.9)\n",
    "        self.replay_buffer = ReplayBuffer(capacity=16200 * 10)\n",
    "\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.96\n",
    "        self.update_target_every = 2\n",
    "\n",
    "    def select_action(self, observation, agent):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self.env.action_space(agent).sample()\n",
    "\n",
    "        observation = (\n",
    "            torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n",
    "        )\n",
    "        self.q_network.eval()\n",
    "        with torch.inference_mode():\n",
    "            q_values = self.q_network(observation)\n",
    "            # print(q_values)\n",
    "        return torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "    def pretrained_action(self, observation):\n",
    "        observation = (\n",
    "            torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n",
    "        )\n",
    "        self.red_pretrained_network.eval()\n",
    "        with torch.inference_mode():\n",
    "            q_values = self.red_pretrained_network(observation)\n",
    "        return torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "    def training(self, episodes=100, batch_size=2 ** 12):        \n",
    "        for episode in range(episodes):\n",
    "            self.env.reset()\n",
    "            \n",
    "            total_reward = 0\n",
    "            reward_for_agent = {agent: 0 for agent in self.env.agents if agent.startswith('blue')}\n",
    "            prev_observation = {}\n",
    "            prev_action = {}\n",
    "            self.env.reset()\n",
    "            step = 0\n",
    "\n",
    "            for idx, agent in enumerate(self.env.agent_iter()):\n",
    "                step += 1\n",
    "                observation, reward, termination, truncation, info = self.env.last()\n",
    "                observation = np.transpose(observation, (2, 0, 1))\n",
    "                \n",
    "                agent_handle = agent.split('_')[0]\n",
    "                \n",
    "                if agent_handle == 'blue':\n",
    "                    total_reward += reward\n",
    "                    reward_for_agent[agent] += reward\n",
    "                    \n",
    "                if termination or truncation:\n",
    "                    action = None\n",
    "                else:\n",
    "                    if agent_handle == 'blue':\n",
    "                        action = self.select_action(observation, agent)\n",
    "                    else:\n",
    "                        action = self.env.action_space(agent).sample()\n",
    "                        # action = self.pretrained_action(observation)\n",
    "\n",
    "                if agent_handle == 'blue':\n",
    "                    prev_observation[agent] = observation\n",
    "                    prev_action[agent] = action\n",
    "                \n",
    "                self.env.step(action)\n",
    "                \n",
    "                if (idx + 1) % self.env.num_agents == 0:\n",
    "                    break\n",
    "                \n",
    "            for agent in self.env.agent_iter():\n",
    "                step += 1\n",
    "                \n",
    "                observation, reward, termination, truncation, info = self.env.last()\n",
    "                observation = np.transpose(observation, (2, 0, 1))\n",
    "                \n",
    "                agent_handle = agent.split('_')[0]\n",
    "                \n",
    "                if agent_handle == 'blue':\n",
    "                    total_reward += reward\n",
    "                    reward_for_agent[agent] += reward\n",
    "                    \n",
    "                if termination or truncation:\n",
    "                    action = None\n",
    "                else:\n",
    "                    if agent_handle == 'blue':\n",
    "                        action = self.select_action(observation, agent)\n",
    "                    else:\n",
    "                        action = self.env.action_space(agent).sample()\n",
    "                        # action = self.pretrained_action(observation)\n",
    "    \n",
    "                    if agent_handle == 'blue':\n",
    "                        self.replay_buffer.add(\n",
    "                            prev_observation[agent],\n",
    "                            prev_action[agent],\n",
    "                            reward,  \n",
    "                            observation,\n",
    "                            termination\n",
    "                        )\n",
    "\n",
    "                        prev_observation[agent] = observation\n",
    "                        prev_action[agent] = action\n",
    "    \n",
    "                self.env.step(action)\n",
    "            \n",
    "            dataloader = DataLoader(self.replay_buffer, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "            self.update_model(dataloader)\n",
    "                \n",
    "            if (episode + 1) % self.update_target_every == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "                self.steplr.step()\n",
    "    \n",
    "            max_reward = max(reward_for_agent.values())\n",
    "            \n",
    "            print(f\"Episode {episode}, Epsilon: {self.epsilon:.2f}, Total Reward: {total_reward}, Steps: {step}, Max Reward: {max_reward}, lr: {self.steplr.get_last_lr()} \")\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def update_model(self, dataloader):\n",
    "        self.q_network.train()\n",
    "        for states, actions, rewards, next_states, dones in dataloader:\n",
    "            # print(states.shape)\n",
    "\n",
    "            states = states.to(dtype=torch.float32, device=self.device)\n",
    "            actions = actions.to(dtype=torch.long, device=self.device)\n",
    "            rewards = rewards.to(dtype=torch.float32, device=self.device)\n",
    "            next_states = next_states.to(dtype=torch.float32, device=self.device)\n",
    "            dones = dones.to(dtype=torch.float32, device=self.device)\n",
    "\n",
    "            current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            with torch.inference_mode():\n",
    "                next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            expected_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "            loss = self.criterion(current_q_values, expected_q_values)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d0646c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:07:38.945257Z",
     "iopub.status.busy": "2024-12-15T05:07:38.945024Z",
     "iopub.status.idle": "2024-12-15T05:20:13.611324Z",
     "shell.execute_reply": "2024-12-15T05:20:13.610377Z"
    },
    "papermill": {
     "duration": 754.674715,
     "end_time": "2024-12-15T05:20:13.613467",
     "exception": false,
     "start_time": "2024-12-15T05:07:38.938752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Epsilon: 1.00, Total Reward: -3230.5251185530797, Steps: 158820, Max Reward: -10.28000044170767, lr: [0.001] \n",
      "Episode 1, Epsilon: 0.96, Total Reward: -2970.395110585727, Steps: 156991, Max Reward: -6.375000230036676, lr: [0.0009000000000000001] \n",
      "Episode 2, Epsilon: 0.92, Total Reward: -3027.7801094865426, Steps: 158342, Max Reward: -11.105000398121774, lr: [0.0009000000000000001] \n",
      "Episode 3, Epsilon: 0.88, Total Reward: -2684.3500985521823, Steps: 154572, Max Reward: -9.10500031709671, lr: [0.0008100000000000001] \n",
      "Episode 4, Epsilon: 0.85, Total Reward: -2746.6701001953334, Steps: 155942, Max Reward: -25.900001152418554, lr: [0.0008100000000000001] \n",
      "Episode 5, Epsilon: 0.82, Total Reward: -1133.7450562650338, Steps: 62657, Max Reward: 1.9549992512911558, lr: [0.000729] \n",
      "Episode 6, Epsilon: 0.78, Total Reward: -2741.9000957002863, Steps: 156103, Max Reward: -24.700001201592386, lr: [0.000729] \n",
      "Episode 7, Epsilon: 0.75, Total Reward: -2325.70009253826, Steps: 118931, Max Reward: -0.6000012028962374, lr: [0.0006561000000000001] \n",
      "Episode 8, Epsilon: 0.72, Total Reward: -2330.2300873557106, Steps: 128091, Max Reward: 2.999998778104782, lr: [0.0006561000000000001] \n",
      "Episode 9, Epsilon: 0.69, Total Reward: -2105.9000836750492, Steps: 97932, Max Reward: 1.3999988324940205, lr: [0.00059049] \n",
      "Episode 10, Epsilon: 0.66, Total Reward: 147.0299884211272, Steps: 15094, Max Reward: 17.029999805614352, lr: [0.00059049] \n",
      "Episode 11, Epsilon: 0.64, Total Reward: -1916.7200763337314, Steps: 94740, Max Reward: 7.799998911097646, lr: [0.000531441] \n",
      "Episode 12, Epsilon: 0.61, Total Reward: -393.5150281842798, Steps: 34465, Max Reward: 15.98499960731715, lr: [0.000531441] \n",
      "Episode 13, Epsilon: 0.59, Total Reward: 194.2649902990088, Steps: 13981, Max Reward: 23.16499980352819, lr: [0.0004782969] \n",
      "Episode 14, Epsilon: 0.56, Total Reward: -1728.5000666631386, Steps: 98727, Max Reward: 18.499999000690877, lr: [0.0004782969] \n",
      "Episode 15, Epsilon: 0.54, Total Reward: 206.95999090187252, Steps: 13165, Max Reward: 22.65499980468303, lr: [0.00043046721] \n",
      "Episode 16, Epsilon: 0.52, Total Reward: -116.1700186189264, Steps: 27910, Max Reward: 23.62999963760376, lr: [0.00043046721] \n",
      "Episode 17, Epsilon: 0.50, Total Reward: 255.43999256193638, Steps: 11670, Max Reward: 23.534999848343432, lr: [0.000387420489] \n",
      "Episode 18, Epsilon: 0.48, Total Reward: 256.2699923934415, Steps: 10898, Max Reward: 22.774999835528433, lr: [0.000387420489] \n",
      "Episode 19, Epsilon: 0.46, Total Reward: 276.48499320447445, Steps: 10133, Max Reward: 28.284999830648303, lr: [0.0003486784401] \n",
      "Episode 20, Epsilon: 0.44, Total Reward: 283.674993570894, Steps: 10354, Max Reward: 24.574999834410846, lr: [0.0003486784401] \n",
      "Episode 21, Epsilon: 0.42, Total Reward: -1277.6000489108264, Steps: 85181, Max Reward: 4.099999305792153, lr: [0.00031381059609000004] \n",
      "Episode 22, Epsilon: 0.41, Total Reward: 320.7799946004525, Steps: 8406, Max Reward: 19.679999880492687, lr: [0.00031381059609000004] \n",
      "Episode 23, Epsilon: 0.39, Total Reward: 297.5199939208105, Steps: 9498, Max Reward: 20.11999988090247, lr: [0.00028242953648100003] \n",
      "Episode 24, Epsilon: 0.38, Total Reward: 295.71499385870993, Steps: 9646, Max Reward: 28.31499981880188, lr: [0.00028242953648100003] \n",
      "Episode 25, Epsilon: 0.36, Total Reward: 333.5099949929863, Steps: 7783, Max Reward: 19.80999988410622, lr: [0.00025418658283290005] \n",
      "Episode 26, Epsilon: 0.35, Total Reward: 260.36999325454235, Steps: 12143, Max Reward: 37.739999753423035, lr: [0.00025418658283290005] \n",
      "Episode 27, Epsilon: 0.33, Total Reward: 323.86999479681253, Steps: 8296, Max Reward: 19.069999888539314, lr: [0.00022876792454961005] \n",
      "Episode 28, Epsilon: 0.32, Total Reward: 326.16999503597617, Steps: 8784, Max Reward: 24.239999871701002, lr: [0.00022876792454961005] \n",
      "Episode 29, Epsilon: 0.31, Total Reward: 339.78999536205083, Steps: 8019, Max Reward: 34.38999984506518, lr: [0.00020589113209464906] \n",
      "Episode 30, Epsilon: 0.29, Total Reward: 348.80499567091465, Steps: 7800, Max Reward: 29.404999865218997, lr: [0.00020589113209464906] \n",
      "Episode 31, Epsilon: 0.28, Total Reward: 335.36499531008303, Steps: 8516, Max Reward: 23.264999845065176, lr: [0.00018530201888518417] \n",
      "Episode 32, Epsilon: 0.27, Total Reward: 352.5149957863614, Steps: 7375, Max Reward: 18.914999878965318, lr: [0.00018530201888518417] \n",
      "Episode 33, Epsilon: 0.26, Total Reward: 344.9699955089018, Steps: 7643, Max Reward: 19.204999903216958, lr: [0.00016677181699666576] \n",
      "Episode 34, Epsilon: 0.25, Total Reward: 347.5949956756085, Steps: 7806, Max Reward: 30.094999856315553, lr: [0.00016677181699666576] \n",
      "Episode 35, Epsilon: 0.24, Total Reward: 317.46999504510313, Steps: 10030, Max Reward: 22.66999984253198, lr: [0.0001500946352969992] \n",
      "Episode 36, Epsilon: 0.23, Total Reward: 353.51999580301344, Steps: 7395, Max Reward: 29.41999985370785, lr: [0.0001500946352969992] \n",
      "Episode 37, Epsilon: 0.22, Total Reward: 288.21499461028725, Steps: 12348, Max Reward: 28.414999813772738, lr: [0.0001350851717672993] \n",
      "Episode 38, Epsilon: 0.21, Total Reward: 342.7799955429509, Steps: 8176, Max Reward: 19.179999875836074, lr: [0.0001350851717672993] \n",
      "Episode 39, Epsilon: 0.20, Total Reward: 343.06999560818076, Steps: 8201, Max Reward: 20.169999885372818, lr: [0.00012157665459056936] \n",
      "Episode 40, Epsilon: 0.20, Total Reward: 364.72999623045325, Steps: 6965, Max Reward: 29.829999859444797, lr: [0.00012157665459056936] \n",
      "Episode 41, Epsilon: 0.19, Total Reward: 365.2299962518737, Steps: 7051, Max Reward: 20.029999875463545, lr: [0.00010941898913151243] \n",
      "Episode 42, Epsilon: 0.18, Total Reward: 363.5349961472675, Steps: 7030, Max Reward: 14.934999919496477, lr: [0.00010941898913151243] \n",
      "Episode 43, Epsilon: 0.17, Total Reward: 357.5299959164113, Steps: 6985, Max Reward: 19.729999873787165, lr: [9.847709021836118e-05] \n",
      "Episode 44, Epsilon: 0.17, Total Reward: 367.8349963314831, Steps: 6986, Max Reward: 20.23499990068376, lr: [9.847709021836118e-05] \n",
      "Episode 45, Epsilon: 0.16, Total Reward: 346.62499600369483, Steps: 8766, Max Reward: 18.12499986216426, lr: [8.862938119652506e-05] \n",
      "Episode 46, Epsilon: 0.15, Total Reward: 370.7399964351207, Steps: 6758, Max Reward: 29.63999987859279, lr: [8.862938119652506e-05] \n",
      "Episode 47, Epsilon: 0.15, Total Reward: 363.73999613523483, Steps: 6856, Max Reward: 19.139999892562628, lr: [7.976644307687256e-05] \n",
      "Episode 48, Epsilon: 0.14, Total Reward: 364.72499625664204, Steps: 7075, Max Reward: 34.92499983217567, lr: [7.976644307687256e-05] \n",
      "Episode 49, Epsilon: 0.14, Total Reward: 370.0299964575097, Steps: 6842, Max Reward: 20.32999991066754, lr: [7.17897987691853e-05] \n",
      "Episode 50, Epsilon: 0.13, Total Reward: 366.2899962179363, Steps: 6764, Max Reward: 19.644999897107482, lr: [7.17897987691853e-05] \n",
      "Episode 51, Epsilon: 0.12, Total Reward: 362.62499616108835, Steps: 7101, Max Reward: 19.724999885074794, lr: [6.461081889226677e-05] \n",
      "Episode 52, Epsilon: 0.12, Total Reward: 375.23999662231654, Steps: 6798, Max Reward: 34.53999984264374, lr: [6.461081889226677e-05] \n",
      "Episode 53, Epsilon: 0.11, Total Reward: 378.5699966121465, Steps: 6202, Max Reward: 19.769999912008643, lr: [5.81497370030401e-05] \n",
      "Episode 54, Epsilon: 0.11, Total Reward: 370.209996570833, Steps: 7306, Max Reward: 19.709999907761812, lr: [5.81497370030401e-05] \n",
      "Episode 55, Epsilon: 0.11, Total Reward: 367.01499640755355, Steps: 7152, Max Reward: 24.714999898336828, lr: [5.233476330273609e-05] \n",
      "Episode 56, Epsilon: 0.10, Total Reward: 385.4799968553707, Steps: 6024, Max Reward: 24.479999884031713, lr: [5.233476330273609e-05] \n",
      "Episode 57, Epsilon: 0.10, Total Reward: 376.03499668836594, Steps: 6804, Max Reward: 30.03499986231327, lr: [4.7101286972462485e-05] \n",
      "Episode 58, Epsilon: 0.10, Total Reward: 379.83999682497233, Steps: 6693, Max Reward: 29.339999865740538, lr: [4.7101286972462485e-05] \n",
      "Episode 59, Epsilon: 0.10, Total Reward: 379.5549967335537, Steps: 6514, Max Reward: 20.25499988347292, lr: [4.239115827521624e-05] \n",
      "Episode 60, Epsilon: 0.10, Total Reward: 375.6299966974184, Steps: 6946, Max Reward: 24.829999868758023, lr: [4.239115827521624e-05] \n",
      "Episode 61, Epsilon: 0.10, Total Reward: 373.51499659754336, Steps: 6862, Max Reward: 25.329999873414636, lr: [3.8152042447694614e-05] \n",
      "Episode 62, Epsilon: 0.10, Total Reward: 374.49999661557376, Steps: 6961, Max Reward: 34.83499983884394, lr: [3.8152042447694614e-05] \n",
      "Episode 63, Epsilon: 0.10, Total Reward: 374.23999658506364, Steps: 6714, Max Reward: 19.739999901503325, lr: [3.433683820292515e-05] \n",
      "Episode 64, Epsilon: 0.10, Total Reward: 373.03999653365463, Steps: 6877, Max Reward: 19.639999891631305, lr: [3.433683820292515e-05] \n",
      "Episode 65, Epsilon: 0.10, Total Reward: 368.9949965979904, Steps: 7546, Max Reward: 19.89499988872558, lr: [3.090315438263264e-05] \n",
      "Episode 66, Epsilon: 0.10, Total Reward: 375.0499965669587, Steps: 6529, Max Reward: 19.84999988321215, lr: [3.090315438263264e-05] \n",
      "Episode 67, Epsilon: 0.10, Total Reward: 372.02499656938016, Steps: 7019, Max Reward: 24.9249998787418, lr: [2.7812838944369376e-05] \n",
      "Episode 68, Epsilon: 0.10, Total Reward: 372.12999654747546, Steps: 6839, Max Reward: 29.829999859444797, lr: [2.7812838944369376e-05] \n",
      "Episode 69, Epsilon: 0.10, Total Reward: 373.90999669302255, Steps: 7133, Max Reward: 19.914999899454415, lr: [2.503155504993244e-05] \n",
      "Episode 70, Epsilon: 0.10, Total Reward: 378.0549966637045, Steps: 6413, Max Reward: 20.15499988477677, lr: [2.503155504993244e-05] \n",
      "Episode 71, Epsilon: 0.10, Total Reward: 375.0499965669587, Steps: 6541, Max Reward: 24.749999875202775, lr: [2.2528399544939195e-05] \n",
      "Episode 72, Epsilon: 0.10, Total Reward: 375.44499661028385, Steps: 6617, Max Reward: 24.64499987103045, lr: [2.2528399544939195e-05] \n",
      "Episode 73, Epsilon: 0.10, Total Reward: 341.23499625362456, Steps: 10019, Max Reward: 18.83499989286065, lr: [2.0275559590445276e-05] \n",
      "Episode 74, Epsilon: 0.10, Total Reward: 372.75499643664807, Steps: 6523, Max Reward: 19.954999898560345, lr: [2.0275559590445276e-05] \n",
      "Episode 75, Epsilon: 0.10, Total Reward: 368.4049965143204, Steps: 7349, Max Reward: 19.90499991644174, lr: [1.8248003631400748e-05] \n",
      "Episode 76, Epsilon: 0.10, Total Reward: 381.0649967454374, Steps: 6289, Max Reward: 24.964999872259796, lr: [1.8248003631400748e-05] \n",
      "Episode 77, Epsilon: 0.10, Total Reward: 376.04499663598835, Steps: 6766, Max Reward: 19.84499990567565, lr: [1.6423203268260675e-05] \n",
      "Episode 78, Epsilon: 0.10, Total Reward: 384.2749968301505, Steps: 6058, Max Reward: 25.374999889172614, lr: [1.6423203268260675e-05] \n",
      "Episode 79, Epsilon: 0.10, Total Reward: 344.5649962378666, Steps: 9597, Max Reward: 19.464999901130795, lr: [1.4780882941434607e-05] \n",
      "Episode 80, Epsilon: 0.10, Total Reward: 378.3649966297671, Steps: 6243, Max Reward: 19.964999892748892, lr: [1.4780882941434607e-05] \n",
      "Episode 81, Epsilon: 0.10, Total Reward: 380.8649967368692, Steps: 6210, Max Reward: 19.76499990094453, lr: [1.3302794647291146e-05] \n",
      "Episode 82, Epsilon: 0.10, Total Reward: 382.009996753186, Steps: 6090, Max Reward: 24.669999892823398, lr: [1.3302794647291146e-05] \n",
      "Episode 83, Epsilon: 0.10, Total Reward: 377.3999967072159, Steps: 6622, Max Reward: 20.13999990746379, lr: [1.1972515182562031e-05] \n",
      "Episode 84, Epsilon: 0.10, Total Reward: 379.85499674640596, Steps: 6481, Max Reward: 24.95499987807125, lr: [1.1972515182562031e-05] \n",
      "Episode 85, Epsilon: 0.10, Total Reward: 380.3649967154488, Steps: 6239, Max Reward: 19.76499988976866, lr: [1.0775263664305828e-05] \n",
      "Episode 86, Epsilon: 0.10, Total Reward: 383.4649968482554, Steps: 6242, Max Reward: 24.664999892935157, lr: [1.0775263664305828e-05] \n",
      "Episode 87, Epsilon: 0.10, Total Reward: 380.16999668069184, Steps: 6130, Max Reward: 24.869999862276018, lr: [9.697737297875246e-06] \n",
      "Episode 88, Epsilon: 0.10, Total Reward: 382.75999683886766, Steps: 6348, Max Reward: 35.05999984126538, lr: [9.697737297875246e-06] \n",
      "Episode 89, Epsilon: 0.10, Total Reward: 379.6649966854602, Steps: 6274, Max Reward: 25.264999885112047, lr: [8.727963568087722e-06] \n",
      "Episode 90, Epsilon: 0.10, Total Reward: 378.48499660380185, Steps: 6133, Max Reward: 15.669999920763075, lr: [8.727963568087722e-06] \n",
      "Episode 91, Epsilon: 0.10, Total Reward: 371.31499659176916, Steps: 7042, Max Reward: 29.814999854192138, lr: [7.85516721127895e-06] \n",
      "Episode 92, Epsilon: 0.10, Total Reward: 375.22999668028206, Steps: 6820, Max Reward: 25.229999874718487, lr: [7.85516721127895e-06] \n",
      "Episode 93, Epsilon: 0.10, Total Reward: 380.5749966716394, Steps: 6112, Max Reward: 19.77499990630895, lr: [7.069650490151056e-06] \n",
      "Episode 94, Epsilon: 0.10, Total Reward: 378.4699966078624, Steps: 6223, Max Reward: 19.46999988798052, lr: [7.069650490151056e-06] \n",
      "Episode 95, Epsilon: 0.10, Total Reward: 373.1849965918809, Steps: 6787, Max Reward: 24.22999986540526, lr: [6.362685441135951e-06] \n",
      "Episode 96, Epsilon: 0.10, Total Reward: 380.66999670211226, Steps: 6093, Max Reward: 19.869999905116856, lr: [6.362685441135951e-06] \n",
      "Episode 97, Epsilon: 0.10, Total Reward: 381.0649967454374, Steps: 6216, Max Reward: 19.964999903924763, lr: [5.7264168970223554e-06] \n",
      "Episode 98, Epsilon: 0.10, Total Reward: 377.2549966350198, Steps: 6454, Max Reward: 19.85499989427626, lr: [5.7264168970223554e-06] \n",
      "Episode 99, Epsilon: 0.10, Total Reward: 376.96999654360116, Steps: 6151, Max Reward: 28.86999983806163, lr: [5.15377520732012e-06] \n"
     ]
    }
   ],
   "source": [
    "env = battle_v4.env(map_size=45, render_mode=None)\n",
    "\n",
    "config_model_qnet = {\n",
    "    'layer_in':{\n",
    "        'in_channels': env.observation_space(\"red_0\").shape[-1],\n",
    "        'out_channels': 64\n",
    "    },\n",
    "    'conv': {\n",
    "        'num_layers': 1,\n",
    "        'layer1':{\n",
    "            'num_blocks': 2,\n",
    "            'in_features': 64,\n",
    "            'outs_features': [128,256,128],\n",
    "        },\n",
    "    },\n",
    "    'mlp': {\n",
    "        'num_layers': 1,\n",
    "        'layer1':{\n",
    "            'num_blocks': 2,\n",
    "            'in_features': 128,\n",
    "            'outs_features': [256,512,128],\n",
    "        },\n",
    "    },\n",
    "    'layer_out':{\n",
    "        'in_features': 128,\n",
    "        'out_features': env.action_space(\"red_0\").n\n",
    "    }\n",
    "}\n",
    "\n",
    "trainer = Trainer(env, config_qnet=None, input_shape=env.observation_space(\"red_0\").shape, action_shape=env.action_space(\"red_0\").n)\n",
    "trainer.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f0a5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:20:13.636019Z",
     "iopub.status.busy": "2024-12-15T05:20:13.635575Z",
     "iopub.status.idle": "2024-12-15T05:20:13.647675Z",
     "shell.execute_reply": "2024-12-15T05:20:13.646685Z"
    },
    "papermill": {
     "duration": 0.024822,
     "end_time": "2024-12-15T05:20:13.649481",
     "exception": false,
     "start_time": "2024-12-15T05:20:13.624659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(trainer.q_network.state_dict(), \"models/blue_resnet_vs_random.pt\")\n",
    "print(\"Training complete. Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0622b310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:20:13.671546Z",
     "iopub.status.busy": "2024-12-15T05:20:13.671190Z",
     "iopub.status.idle": "2024-12-15T05:20:13.683946Z",
     "shell.execute_reply": "2024-12-15T05:20:13.682844Z"
    },
    "papermill": {
     "duration": 0.025614,
     "end_time": "2024-12-15T05:20:13.685712",
     "exception": false,
     "start_time": "2024-12-15T05:20:13.660098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class finalQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 120),\n",
    "            # nn.LayerNorm(120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            # nn.LayerNorm(84),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.last_layer = nn.Linear(84, action_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        x = self.network(x)\n",
    "        self.last_latent = x\n",
    "        return self.last_layer(x)\n",
    "\n",
    "class vQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, action_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84da854e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:20:13.707907Z",
     "iopub.status.busy": "2024-12-15T05:20:13.707198Z",
     "iopub.status.idle": "2024-12-15T05:20:18.356349Z",
     "shell.execute_reply": "2024-12-15T05:20:18.355390Z"
    },
    "papermill": {
     "duration": 4.662301,
     "end_time": "2024-12-15T05:20:18.358265",
     "exception": false,
     "start_time": "2024-12-15T05:20:13.695964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done recording pretrained agents\n"
     ]
    }
   ],
   "source": [
    "# make video\n",
    "import cv2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "env = battle_v4.env(map_size=45, render_mode=\"rgb_array\", max_cycles=300)\n",
    "vid_dir = \"video\"\n",
    "os.makedirs(vid_dir, exist_ok=True)\n",
    "fps = 35\n",
    "frames = []\n",
    "my_q_network = MyQNetwork(observation_shape=env.observation_space(\"red_0\").shape, action_shape=env.action_space(\"red_0\").n)\n",
    "\n",
    "my_q_network.load_state_dict(\n",
    "    torch.load(\"/kaggle/working/models/blue_resnet_vs_random.pt\", weights_only=True, map_location=\"cpu\")\n",
    ")\n",
    "my_q_network.to(device)\n",
    "my_q_network.eval()\n",
    "\n",
    "red_final_network = finalQNetwork(\n",
    "    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    ")\n",
    "red_final_network.load_state_dict(\n",
    "    torch.load(\"/kaggle/working/RL-final-project-AIT-3007/red_final.pt\", weights_only=True, map_location=\"cpu\")\n",
    ")\n",
    "red_final_network.to(device)\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    observation = (\n",
    "        torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "    )\n",
    "    if termination or truncation:\n",
    "        action = None  # this agent has died\n",
    "    else:\n",
    "        agent_handle = agent.split(\"_\")[0]\n",
    "        if agent_handle == \"blue\":\n",
    "            with torch.inference_mode():\n",
    "                q_values = my_q_network(observation)\n",
    "            action = torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "        else:\n",
    "            # action = env.action_space(agent).sample()\n",
    "            with torch.inference_mode():\n",
    "                q_values = red_final_network(observation)\n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "            \n",
    "    env.step(action)\n",
    "\n",
    "    if agent == \"red_0\":\n",
    "        frames.append(env.render())\n",
    "\n",
    "height, width, _ = frames[0].shape\n",
    "out = cv2.VideoWriter(\n",
    "    os.path.join(vid_dir, f\"blue_vs_red_final.mp4\"),\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    fps,\n",
    "    (width, height),\n",
    ")\n",
    "for frame in frames:\n",
    "    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    out.write(frame_bgr)\n",
    "out.release()\n",
    "print(\"Done recording pretrained agents\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4653904a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T05:20:18.380521Z",
     "iopub.status.busy": "2024-12-15T05:20:18.380229Z",
     "iopub.status.idle": "2024-12-15T05:27:28.805146Z",
     "shell.execute_reply": "2024-12-15T05:27:28.804040Z"
    },
    "papermill": {
     "duration": 430.437927,
     "end_time": "2024-12-15T05:27:28.806856",
     "exception": false,
     "start_time": "2024-12-15T05:20:18.368929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:72: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Eval with final policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:02<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.0, 'winrate_blue': 1.0, 'average_rewards_red': 2.668094631581886, 'average_rewards_blue': 4.757100790606066}\n",
      "====================\n",
      "Eval with pretrain policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:26<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.0, 'winrate_blue': 1.0, 'average_rewards_red': 0.4954917650918933, 'average_rewards_blue': 4.881615192421671}\n",
      "====================\n",
      "Eval with v1 policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:46<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.3, 'winrate_blue': 0.5666666666666667, 'average_rewards_red': 4.166713969997004, 'average_rewards_blue': 4.5464691039914475}\n",
      "====================\n",
      "Eval with v2 policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:53<00:00,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.23333333333333334, 'winrate_blue': 0.4666666666666667, 'average_rewards_red': 4.381973228227239, 'average_rewards_blue': 4.449320954714476}\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from torch_model import QNetwork\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, *args, **kwargs: x  # Fallback: tqdm becomes a no-op\n",
    "    \n",
    "def eval():\n",
    "    max_cycles = 300\n",
    "    env = battle_v4.env(map_size=45, max_cycles=max_cycles)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def random_policy(env, agent, obs):\n",
    "        return env.action_space(agent).sample()\n",
    "\n",
    "    # MyNetwork = QNet(config_model_qnet, device=device)\n",
    "    MyNetwork = MyQNetwork(observation_shape=env.observation_space(\"red_0\").shape, action_shape=env.action_space(\"red_0\").n)\n",
    "    \n",
    "    MyNetwork.load_state_dict(\n",
    "        torch.load(\"/kaggle/working/models/blue_resnet_vs_random.pt\", weights_only=True, map_location=\"cpu\")\n",
    "    )\n",
    "    MyNetwork.to(device)\n",
    "\n",
    "    # red_final_network = vQNetwork(\n",
    "    #     env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    # )\n",
    "    # red_final_network.load_state_dict(\n",
    "    #     torch.load(\"/kaggle/working/RL-final-project-AIT-3007/red_final.pt\", weights_only=True, map_location=\"cpu\")\n",
    "    # ).to(device)\n",
    "\n",
    "    red_final_network = finalQNetwork(\n",
    "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    )\n",
    "    red_final_network.load_state_dict(\n",
    "        torch.load(\"/kaggle/working/RL-final-project-AIT-3007/red_final.pt\", weights_only=True, map_location=\"cpu\")\n",
    "    )\n",
    "    red_final_network = red_final_network.to(device)\n",
    "\n",
    "    ###\n",
    "    red_pretrained_network = vQNetwork(\n",
    "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    )\n",
    "    red_pretrained_network.load_state_dict(\n",
    "        torch.load(\"/kaggle/working/RL-final-project-AIT-3007/red.pt\", weights_only=True, map_location=\"cpu\")\n",
    "    )\n",
    "    red_pretrained_network = red_pretrained_network.to(device)\n",
    "\n",
    "    ###\n",
    "    v1 = vQNetwork(\n",
    "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    )\n",
    "    v1.load_state_dict(\n",
    "        torch.load(\"/kaggle/input/another-model/v1.pth\", weights_only=True, map_location=\"cpu\")\n",
    "    )\n",
    "    v1 = v1.to(device)\n",
    "\n",
    "    ###\n",
    "    v2 = vQNetwork(\n",
    "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    )\n",
    "    v2.load_state_dict(\n",
    "        torch.load(\"/kaggle/input/another-model/v2.pth\", weights_only=True, map_location=\"cpu\")\n",
    "    )\n",
    "    v2 = v2.to(device)\n",
    "    \n",
    "    def pretrain_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        red_pretrained_network.eval()\n",
    "        with torch.inference_mode():\n",
    "            q_values = red_pretrained_network(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "    def pretrain_policy_final(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        red_final_network.eval()\n",
    "        with torch.inference_mode():\n",
    "            q_values = red_final_network(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "    def v1_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        v1.eval()\n",
    "        with torch.inference_mode():\n",
    "            q_values = v1(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    def v2_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        v2.eval()\n",
    "        with torch.inference_mode():\n",
    "            q_values = v2(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    def my_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        MyNetwork.eval()\n",
    "        with torch.inference_mode():\n",
    "            q_values = MyNetwork(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    # def policy()\n",
    "\n",
    "    def run_eval(env, red_policy, blue_policy, n_episode: int = 100):\n",
    "        red_win, blue_win = [], []\n",
    "        red_tot_rw, blue_tot_rw = [], []\n",
    "        n_agent_each_team = len(env.env.action_spaces) // 2\n",
    "\n",
    "        for _ in tqdm(range(n_episode)):\n",
    "            env.reset()\n",
    "            n_kill = {\"red\": 0, \"blue\": 0}\n",
    "            red_reward, blue_reward = 0, 0\n",
    "\n",
    "            for agent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "                agent_team = agent.split(\"_\")[0]\n",
    "\n",
    "                n_kill[agent_team] += (\n",
    "                    reward > 4.5\n",
    "                )  # This assumes default reward settups\n",
    "                if agent_team == \"red\":\n",
    "                    red_reward += reward\n",
    "                else:\n",
    "                    blue_reward += reward\n",
    "\n",
    "                if termination or truncation:\n",
    "                    action = None  # this agent has died\n",
    "                else:\n",
    "                    if agent_team == \"red\":\n",
    "                        action = red_policy(env, agent, observation)\n",
    "                    else:\n",
    "                        action = blue_policy(env, agent, observation)\n",
    "\n",
    "                env.step(action)\n",
    "\n",
    "            who_wins = \"red\" if n_kill[\"red\"] >= n_kill[\"blue\"] + 5 else \"draw\"\n",
    "            who_wins = \"blue\" if n_kill[\"red\"] + 5 <= n_kill[\"blue\"] else who_wins\n",
    "            red_win.append(who_wins == \"red\")\n",
    "            blue_win.append(who_wins == \"blue\")\n",
    "\n",
    "            red_tot_rw.append(red_reward / n_agent_each_team)\n",
    "            blue_tot_rw.append(blue_reward / n_agent_each_team)\n",
    "\n",
    "        return {\n",
    "            \"winrate_red\": np.mean(red_win),\n",
    "            \"winrate_blue\": np.mean(blue_win),\n",
    "            \"average_rewards_red\": np.mean(red_tot_rw),\n",
    "            \"average_rewards_blue\": np.mean(blue_tot_rw),\n",
    "        }\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Eval with final policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=pretrain_policy_final, blue_policy=my_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Eval with pretrain policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=pretrain_policy, blue_policy=my_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Eval with v1 policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=v1_policy, blue_policy=my_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    print(\"Eval with v2 policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=v2_policy, blue_policy=my_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    eval()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6273654,
     "sourceId": 10160102,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6282299,
     "sourceId": 10171873,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6286439,
     "sourceId": 10177726,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6301521,
     "sourceId": 10203097,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1237.13451,
   "end_time": "2024-12-15T05:27:31.449963",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-15T05:06:54.315453",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
