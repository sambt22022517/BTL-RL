{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T14:10:06.856835Z",
     "iopub.status.busy": "2024-12-15T14:10:06.856247Z",
     "iopub.status.idle": "2024-12-15T14:10:41.142059Z",
     "shell.execute_reply": "2024-12-15T14:10:41.140872Z",
     "shell.execute_reply.started": "2024-12-15T14:10:06.856799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Farama-Foundation/MAgent2\n",
      "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-b6af_q0_\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-b6af_q0_\n",
      "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.26.4)\n",
      "Collecting pygame>=2.1.0 (from magent2==0.3.3)\n",
      "  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pettingzoo>=1.23.1 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.24.0)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (0.29.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\n",
      "Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: magent2\n",
      "  Building wheel for magent2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696136 sha256=d3ca8842de76a311865050b8026c00c8fd92a9e885c983b1d9f19ced8912a886\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5j_vl7xh/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\n",
      "Successfully built magent2\n",
      "Installing collected packages: pygame, magent2\n",
      "Successfully installed magent2-0.3.3 pygame-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/Farama-Foundation/MAgent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:10:41.144217Z",
     "iopub.status.busy": "2024-12-15T14:10:41.143924Z",
     "iopub.status.idle": "2024-12-15T14:10:44.106139Z",
     "shell.execute_reply": "2024-12-15T14:10:44.105283Z",
     "shell.execute_reply.started": "2024-12-15T14:10:41.144187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RL-final-project-AIT-3007'...\n",
      "remote: Enumerating objects: 56, done.\u001b[K\n",
      "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 56 (delta 12), reused 13 (delta 6), pack-reused 32 (from 1)\u001b[K\n",
      "Receiving objects: 100% (56/56), 17.78 MiB | 20.02 MiB/s, done.\n",
      "Resolving deltas: 100% (23/23), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/giangbang/RL-final-project-AIT-3007.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:10:44.107949Z",
     "iopub.status.busy": "2024-12-15T14:10:44.107562Z",
     "iopub.status.idle": "2024-12-15T14:10:44.112027Z",
     "shell.execute_reply": "2024-12-15T14:10:44.111253Z",
     "shell.execute_reply.started": "2024-12-15T14:10:44.107906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/RL-final-project-AIT-3007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:10:44.114048Z",
     "iopub.status.busy": "2024-12-15T14:10:44.113801Z",
     "iopub.status.idle": "2024-12-15T14:10:47.536699Z",
     "shell.execute_reply": "2024-12-15T14:10:47.535829Z",
     "shell.execute_reply.started": "2024-12-15T14:10:44.114024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, Counter\n",
    "import os\n",
    "from magent2.environments import battle_v4\n",
    "import time\n",
    "# from torch_model import QNetwork\n",
    "\n",
    "class MyQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], 13, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(13, 13, kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ReplayBuffer(Dataset):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.buffer[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:10:47.538876Z",
     "iopub.status.busy": "2024-12-15T14:10:47.538242Z",
     "iopub.status.idle": "2024-12-15T14:10:47.556981Z",
     "shell.execute_reply": "2024-12-15T14:10:47.556121Z",
     "shell.execute_reply.started": "2024-12-15T14:10:47.538834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, env, input_shape, action_shape, learning_rate=1e-3):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_network = MyQNetwork(input_shape, action_shape).to(self.device)\n",
    "        self.target_network = MyQNetwork(input_shape, action_shape).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.replay_buffer = ReplayBuffer(capacity=16200 * 10)\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9\n",
    "        self.update_target_every = 2\n",
    "\n",
    "    def select_action(self, observation, agent):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self.env.action_space(agent).sample()\n",
    "\n",
    "        observation = (\n",
    "            torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n",
    "        )\n",
    "        with torch.inference_mode():\n",
    "            q_values = self.q_network(observation)\n",
    "        return torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "    def training(self, episodes=50, batch_size=2 ** 10):        \n",
    "        for episode in range(episodes):\n",
    "            self.env.reset()\n",
    "            \n",
    "            total_reward = 0\n",
    "            reward_for_agent = {agent: 0 for agent in self.env.agents if agent.startswith('blue')}\n",
    "            prev_observation = {}\n",
    "            prev_action = {}\n",
    "            self.env.reset()\n",
    "            step = 0\n",
    "\n",
    "            for idx, agent in enumerate(self.env.agent_iter()):\n",
    "                step += 1\n",
    "                observation, reward, termination, truncation, info = self.env.last()\n",
    "                observation = np.transpose(observation, (2, 0, 1))\n",
    "                \n",
    "                agent_handle = agent.split('_')[0]\n",
    "                \n",
    "                if agent_handle == 'blue':\n",
    "                    total_reward += reward\n",
    "                    reward_for_agent[agent] += reward\n",
    "                    \n",
    "                if termination or truncation:\n",
    "                    action = None\n",
    "                else:\n",
    "                    if agent_handle == 'blue':\n",
    "                        action = self.select_action(observation, agent)\n",
    "                    else:\n",
    "                        action = self.env.action_space(agent).sample()\n",
    "\n",
    "                if agent_handle == 'blue':\n",
    "                    prev_observation[agent] = observation\n",
    "                    prev_action[agent] = action\n",
    "                \n",
    "                self.env.step(action)\n",
    "                \n",
    "                if (idx + 1) % self.env.num_agents == 0:\n",
    "                    break\n",
    "                \n",
    "            for agent in self.env.agent_iter():\n",
    "                step += 1\n",
    "                \n",
    "                observation, reward, termination, truncation, info = self.env.last()\n",
    "                observation = np.transpose(observation, (2, 0, 1))\n",
    "                \n",
    "                agent_handle = agent.split('_')[0]\n",
    "                \n",
    "                if agent_handle == 'blue':\n",
    "                    total_reward += reward\n",
    "                    reward_for_agent[agent] += reward\n",
    "                    \n",
    "                if termination or truncation:\n",
    "                    action = None\n",
    "                else:\n",
    "                    if agent_handle == 'blue':\n",
    "                        action = self.select_action(observation, agent)\n",
    "                    else:\n",
    "                        action = self.env.action_space(agent).sample()\n",
    "    \n",
    "                    if agent_handle == 'blue':\n",
    "                        self.replay_buffer.add(\n",
    "                            prev_observation[agent],\n",
    "                            prev_action[agent],\n",
    "                            reward,  \n",
    "                            observation,\n",
    "                            termination\n",
    "                        )\n",
    "\n",
    "                        prev_observation[agent] = observation\n",
    "                        prev_action[agent] = action\n",
    "    \n",
    "                self.env.step(action)\n",
    "            \n",
    "            dataloader = DataLoader(self.replay_buffer, batch_size=batch_size, shuffle=True)\n",
    "            self.update_model(dataloader)\n",
    "                \n",
    "            if (episode + 1) % self.update_target_every == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "            max_reward = max(reward_for_agent.values())\n",
    "            \n",
    "            print(f\"Episode {episode}, Epsilon: {self.epsilon:.2f}, Total Reward: {total_reward}, Steps: {step}, Max Reward: {max_reward} \")\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def update_model(self, dataloader):\n",
    "        for states, actions, rewards, next_states, dones in dataloader:\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            with torch.inference_mode():\n",
    "                next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            expected_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "            loss = self.criterion(current_q_values, expected_q_values)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:10:47.558330Z",
     "iopub.status.busy": "2024-12-15T14:10:47.558057Z",
     "iopub.status.idle": "2024-12-15T14:22:50.312403Z",
     "shell.execute_reply": "2024-12-15T14:22:50.311491Z",
     "shell.execute_reply.started": "2024-12-15T14:10:47.558299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/2370074755.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
      "/tmp/ipykernel_30/2370074755.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
      "/tmp/ipykernel_30/2370074755.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
      "/tmp/ipykernel_30/2370074755.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
      "/tmp/ipykernel_30/2370074755.py:122: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Epsilon: 1.00, Total Reward: -3266.5901186224073, Steps: 161184, Max Reward: -7.990000298246741 \n",
      "Episode 1, Epsilon: 0.90, Total Reward: -2944.3601070651785, Steps: 158089, Max Reward: -9.960000389255583 \n",
      "Episode 2, Epsilon: 0.81, Total Reward: -2372.090096999891, Steps: 116736, Max Reward: -4.465000241063535 \n",
      "Episode 3, Epsilon: 0.73, Total Reward: -2343.700088308193, Steps: 126187, Max Reward: -0.20000114664435387 \n",
      "Episode 4, Epsilon: 0.66, Total Reward: -2223.100078023039, Steps: 140005, Max Reward: -17.500000898726285 \n",
      "Episode 5, Epsilon: 0.59, Total Reward: -1884.2900680135936, Steps: 122853, Max Reward: -1.7700000759214163 \n",
      "Episode 6, Epsilon: 0.53, Total Reward: -1874.3000619122759, Steps: 150544, Max Reward: -4.600000815466046 \n",
      "Episode 7, Epsilon: 0.48, Total Reward: -1766.3000552961603, Steps: 155133, Max Reward: 15.399999102577567 \n",
      "Episode 8, Epsilon: 0.43, Total Reward: -1295.400049522519, Steps: 91150, Max Reward: 23.39999925531447 \n",
      "Episode 9, Epsilon: 0.39, Total Reward: -1374.1000430146232, Steps: 141276, Max Reward: 103.99999889731407 \n",
      "Episode 10, Epsilon: 0.35, Total Reward: 324.1749947723001, Steps: 8593, Max Reward: 24.074999851174653 \n",
      "Episode 11, Epsilon: 0.31, Total Reward: 325.9749948605895, Steps: 8548, Max Reward: 29.274999828077853 \n",
      "Episode 12, Epsilon: 0.28, Total Reward: 315.16999441292137, Steps: 8366, Max Reward: 23.46999984793365 \n",
      "Episode 13, Epsilon: 0.25, Total Reward: 56.49998952075839, Steps: 27349, Max Reward: 12.299999870359898 \n",
      "Episode 14, Epsilon: 0.23, Total Reward: -42.455011871643364, Steps: 39281, Max Reward: 28.644999623298645 \n",
      "Episode 15, Epsilon: 0.21, Total Reward: -649.3000217201188, Steps: 85730, Max Reward: 3.9999997541308403 \n",
      "Episode 16, Epsilon: 0.19, Total Reward: 350.5599959930405, Steps: 8394, Max Reward: 29.959999847225845 \n",
      "Episode 17, Epsilon: 0.17, Total Reward: 362.9449960635975, Steps: 6886, Max Reward: 24.444999868050218 \n",
      "Episode 18, Epsilon: 0.15, Total Reward: -459.1900141434744, Steps: 83731, Max Reward: 7.999999707564712 \n",
      "Episode 19, Epsilon: 0.14, Total Reward: 357.26499625388533, Steps: 7961, Max Reward: 18.864999880082905 \n",
      "Episode 20, Epsilon: 0.12, Total Reward: -360.55500992015004, Steps: 83589, Max Reward: 6.799999874085188 \n",
      "Episode 21, Epsilon: 0.11, Total Reward: 91.18499386683106, Steps: 36257, Max Reward: 21.934999861754477 \n",
      "Episode 22, Epsilon: 0.10, Total Reward: -309.4200078435242, Steps: 89745, Max Reward: 50.59999942034483 \n",
      "Episode 23, Epsilon: 0.09, Total Reward: -263.9500058470294, Steps: 90972, Max Reward: 28.43499969597906 \n",
      "Episode 24, Epsilon: 0.08, Total Reward: -231.1250045672059, Steps: 86895, Max Reward: 40.09999973047525 \n",
      "Episode 25, Epsilon: 0.07, Total Reward: 324.29499710164964, Steps: 16766, Max Reward: 45.109999754466116 \n",
      "Episode 26, Epsilon: 0.06, Total Reward: 384.1099974960089, Steps: 9008, Max Reward: 29.444999859668314 \n",
      "Episode 27, Epsilon: 0.06, Total Reward: -184.60000215284526, Steps: 87173, Max Reward: 36.79999968409538 \n",
      "Episode 28, Epsilon: 0.05, Total Reward: 381.06999684590846, Steps: 6686, Max Reward: 14.944999924860895 \n",
      "Episode 29, Epsilon: 0.05, Total Reward: 363.499996336177, Steps: 7433, Max Reward: 19.89999988861382 \n",
      "Episode 30, Epsilon: 0.04, Total Reward: 383.1549968877807, Steps: 6409, Max Reward: 19.354999895207584 \n",
      "Episode 31, Epsilon: 0.04, Total Reward: 369.7649968927726, Steps: 8602, Max Reward: 19.739999902434647 \n",
      "Episode 32, Epsilon: 0.03, Total Reward: 375.30499680992216, Steps: 7347, Max Reward: 24.604999899864197 \n",
      "Episode 33, Epsilon: 0.03, Total Reward: 366.48499674815685, Steps: 8616, Max Reward: 14.944999931380153 \n",
      "Episode 34, Epsilon: 0.03, Total Reward: 368.2799971746281, Steps: 9405, Max Reward: 19.779999908059835 \n",
      "Episode 35, Epsilon: 0.03, Total Reward: 356.2299968311563, Steps: 10559, Max Reward: 28.039999845437706 \n",
      "Episode 36, Epsilon: 0.02, Total Reward: 377.5149970771745, Steps: 8064, Max Reward: 20.16999990772456 \n",
      "Episode 37, Epsilon: 0.02, Total Reward: 370.3299969760701, Steps: 9054, Max Reward: 24.82999987527728 \n",
      "Episode 38, Epsilon: 0.02, Total Reward: 375.0949969543144, Steps: 8008, Max Reward: 20.374999899417162 \n",
      "Episode 39, Epsilon: 0.02, Total Reward: 379.964997231029, Steps: 8124, Max Reward: 20.459999887272716 \n",
      "Episode 40, Epsilon: 0.01, Total Reward: 378.7649974776432, Steps: 9435, Max Reward: 20.899999893270433 \n",
      "Episode 41, Epsilon: 0.01, Total Reward: 374.8949974095449, Steps: 9821, Max Reward: 19.884999906644225 \n",
      "Episode 42, Epsilon: 0.01, Total Reward: 370.0099970512092, Steps: 9544, Max Reward: 25.809999840334058 \n",
      "Episode 43, Epsilon: 0.01, Total Reward: 375.5899971462786, Steps: 8636, Max Reward: 19.6399999037385 \n",
      "Episode 44, Epsilon: 0.01, Total Reward: 371.44999693520367, Steps: 8714, Max Reward: 19.649999914690852 \n",
      "Episode 45, Epsilon: 0.01, Total Reward: 363.58499741367996, Steps: 11819, Max Reward: 25.084999862127006 \n",
      "Episode 46, Epsilon: 0.01, Total Reward: 359.1599978590384, Steps: 13152, Max Reward: 14.6599999377504 \n",
      "Episode 47, Epsilon: 0.01, Total Reward: 364.83499703370035, Steps: 9619, Max Reward: 15.474999913945794 \n",
      "Episode 48, Epsilon: 0.01, Total Reward: 367.6899974802509, Steps: 11240, Max Reward: 19.094999917782843 \n",
      "Episode 49, Epsilon: 0.01, Total Reward: 377.3149970108643, Steps: 8028, Max Reward: 19.474999911151826 \n"
     ]
    }
   ],
   "source": [
    "env = battle_v4.env(map_size=45, render_mode=None)\n",
    "trainer = Trainer(env, env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n)\n",
    "trainer.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:22:50.313737Z",
     "iopub.status.busy": "2024-12-15T14:22:50.313388Z",
     "iopub.status.idle": "2024-12-15T14:22:50.324698Z",
     "shell.execute_reply": "2024-12-15T14:22:50.323842Z",
     "shell.execute_reply.started": "2024-12-15T14:22:50.313703Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(trainer.q_network.state_dict(), \"models/blue_vs_random.pt\")\n",
    "print(\"Training complete. Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:25:05.284270Z",
     "iopub.status.busy": "2024-12-15T14:25:05.283901Z",
     "iopub.status.idle": "2024-12-15T14:25:08.724986Z",
     "shell.execute_reply": "2024-12-15T14:25:08.724063Z",
     "shell.execute_reply.started": "2024-12-15T14:25:05.284238Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done recording My blue agents vs random\n"
     ]
    }
   ],
   "source": [
    "# make video\n",
    "import cv2\n",
    "\n",
    "env = battle_v4.env(map_size=45, render_mode=\"rgb_array\", max_cycles=300)\n",
    "vid_dir = \"video\"\n",
    "os.makedirs(vid_dir, exist_ok=True)\n",
    "fps = 35\n",
    "frames = []\n",
    "my_q_network = MyQNetwork(\n",
    "    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    ")\n",
    "my_q_network.load_state_dict(\n",
    "    torch.load(\"/kaggle/working/models/blue_vs_random.pt\", weights_only=True, map_location=\"cpu\")\n",
    ")\n",
    "my_q_network.eval()\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    observation = (\n",
    "        torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0)\n",
    "    )\n",
    "    if termination or truncation:\n",
    "        action = None  # this agent has died\n",
    "    else:\n",
    "        agent_handle = agent.split(\"_\")[0]\n",
    "        if agent_handle == \"blue\":\n",
    "            with torch.inference_mode():\n",
    "                q_values = my_q_network(observation)\n",
    "            action = torch.argmax(q_values, dim=1).numpy()[0]\n",
    "        else:\n",
    "            action = env.action_space(agent).sample()\n",
    "            \n",
    "    env.step(action)\n",
    "\n",
    "    if agent == \"red_0\":\n",
    "        frames.append(env.render())\n",
    "\n",
    "height, width, _ = frames[0].shape\n",
    "out = cv2.VideoWriter(\n",
    "    os.path.join(vid_dir, f\"blue_vs_random.mp4\"),\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    fps,\n",
    "    (width, height),\n",
    ")\n",
    "for frame in frames:\n",
    "    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    out.write(frame_bgr)\n",
    "out.release()\n",
    "print(\"Done recording My blue agents vs random\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:26:08.133314Z",
     "iopub.status.busy": "2024-12-15T14:26:08.132605Z",
     "iopub.status.idle": "2024-12-15T14:36:50.150724Z",
     "shell.execute_reply": "2024-12-15T14:36:50.149716Z",
     "shell.execute_reply.started": "2024-12-15T14:26:08.133280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:72: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Eval with pretrain policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:40<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.0, 'winrate_blue': 1.0, 'average_rewards_red': 0.7385699528127663, 'average_rewards_blue': 4.133952614933313}\n",
      "====================\n",
      "Eval with final policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:56<00:00,  7.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.0, 'winrate_blue': 1.0, 'average_rewards_red': 1.7948497787010647, 'average_rewards_blue': 0.3590224500645889}\n",
      "====================\n",
      "Eval with v1 policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:00<00:00,  6.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.06666666666666667, 'winrate_blue': 0.8666666666666667, 'average_rewards_red': 3.799790105453612, 'average_rewards_blue': 2.884337363726701}\n",
      "====================\n",
      "Eval with v2 policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:03<00:00,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winrate_red': 0.6333333333333333, 'winrate_blue': 0.16666666666666666, 'average_rewards_red': 4.637506149958727, 'average_rewards_blue': 3.3422694900161267}\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_model import QNetwork as Q_pretrain\n",
    "from final_torch_model import QNetwork as Q_final\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, *args, **kwargs: x  # Fallback: tqdm becomes a no-op\n",
    "    \n",
    "def eval():\n",
    "    max_cycles = 300\n",
    "    env = battle_v4.env(map_size=45, max_cycles=max_cycles)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def random_policy(env, agent, obs):\n",
    "        return env.action_space(agent).sample()\n",
    "\n",
    "    MyNetwork = MyQNetwork(\n",
    "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    )\n",
    "    MyNetwork.load_state_dict(\n",
    "        torch.load(\"/kaggle/input/blue-improved/blue_improved.pt\", weights_only=True, map_location='cpu')\n",
    "    )\n",
    "    MyNetwork.to(device)\n",
    "\n",
    "    red_pretrained = Q_pretrain(\n",
    "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    )\n",
    "    red_pretrained.load_state_dict(\n",
    "        torch.load(\"/kaggle/working/RL-final-project-AIT-3007/red.pt\", weights_only=True, map_location=\"cpu\")\n",
    "    )\n",
    "    red_pretrained.to(device)\n",
    "\n",
    "    red_final = Q_final(\n",
    "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    )\n",
    "    red_final.load_state_dict(\n",
    "        torch.load('/kaggle/working/RL-final-project-AIT-3007/red_final.pt', weights_only=True, map_location='cpu')\n",
    "    )\n",
    "    red_final.to(device)\n",
    "    \n",
    "    v1 = Q_pretrain(\n",
    "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    )\n",
    "    v1.load_state_dict(\n",
    "        torch.load(\"/kaggle/input/another-model/v1.pth\", weights_only=True, map_location=\"cpu\")\n",
    "    )\n",
    "    v1.to(device)\n",
    "\n",
    "    v2 = Q_pretrain(\n",
    "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    "    )\n",
    "    v2.load_state_dict(\n",
    "        torch.load(\"/kaggle/input/another-model/v2.pth\", weights_only=True, map_location=\"cpu\")\n",
    "    )\n",
    "    v2.to(device)\n",
    "\n",
    "    def pretrain_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        with torch.inference_mode():\n",
    "            q_values = red_pretrained(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    def final_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        with torch.inference_mode():\n",
    "            q_values = red_final(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "    def v1_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        with torch.inference_mode():\n",
    "            q_values = v1(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    def v2_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        with torch.inference_mode():\n",
    "            q_values = v2(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    def my_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        with torch.inference_mode():\n",
    "            q_values = MyNetwork(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    def run_eval(env, red_policy, blue_policy, n_episode: int = 100):\n",
    "        red_win, blue_win = [], []\n",
    "        red_tot_rw, blue_tot_rw = [], []\n",
    "        n_agent_each_team = len(env.env.action_spaces) // 2\n",
    "\n",
    "        for _ in tqdm(range(n_episode)):\n",
    "            env.reset()\n",
    "            n_kill = {\"red\": 0, \"blue\": 0}\n",
    "            red_reward, blue_reward = 0, 0\n",
    "\n",
    "            for agent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "                agent_team = agent.split(\"_\")[0]\n",
    "\n",
    "                n_kill[agent_team] += (\n",
    "                    reward > 4.5\n",
    "                )  # This assumes default reward settups\n",
    "                if agent_team == \"red\":\n",
    "                    red_reward += reward\n",
    "                else:\n",
    "                    blue_reward += reward\n",
    "\n",
    "                if termination or truncation:\n",
    "                    action = None  # this agent has died\n",
    "                else:\n",
    "                    if agent_team == \"red\":\n",
    "                        action = red_policy(env, agent, observation)\n",
    "                    else:\n",
    "                        action = blue_policy(env, agent, observation)\n",
    "\n",
    "                env.step(action)\n",
    "\n",
    "            who_wins = \"red\" if n_kill[\"red\"] >= n_kill[\"blue\"] + 5 else \"draw\"\n",
    "            who_wins = \"blue\" if n_kill[\"red\"] + 5 <= n_kill[\"blue\"] else who_wins\n",
    "            red_win.append(who_wins == \"red\")\n",
    "            blue_win.append(who_wins == \"blue\")\n",
    "\n",
    "            red_tot_rw.append(red_reward / n_agent_each_team)\n",
    "            blue_tot_rw.append(blue_reward / n_agent_each_team)\n",
    "\n",
    "        return {\n",
    "            \"winrate_red\": np.mean(red_win),\n",
    "            \"winrate_blue\": np.mean(blue_win),\n",
    "            \"average_rewards_red\": np.mean(red_tot_rw),\n",
    "            \"average_rewards_blue\": np.mean(blue_tot_rw),\n",
    "        }\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Eval with pretrain policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=pretrain_policy, blue_policy=my_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Eval with final policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=final_policy, blue_policy=my_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Eval with v1 policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=v1_policy, blue_policy=my_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    print(\"Eval with v2 policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=v2_policy, blue_policy=my_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    eval()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6273654,
     "sourceId": 10160102,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6282299,
     "sourceId": 10171873,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6286439,
     "sourceId": 10177726,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
