{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10160102,"sourceType":"datasetVersion","datasetId":6273654},{"sourceId":10171778,"sourceType":"datasetVersion","datasetId":6282226},{"sourceId":10171873,"sourceType":"datasetVersion","datasetId":6282299}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/Farama-Foundation/MAgent2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:49:37.377483Z","iopub.execute_input":"2024-12-12T06:49:37.377891Z","iopub.status.idle":"2024-12-12T06:50:11.574094Z","shell.execute_reply.started":"2024-12-12T06:49:37.377840Z","shell.execute_reply":"2024-12-12T06:50:11.572967Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Farama-Foundation/MAgent2\n  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-w9twgot3\n  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-w9twgot3\n  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.26.4)\nCollecting pygame>=2.1.0 (from magent2==0.3.3)\n  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: pettingzoo>=1.23.1 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.24.0)\nRequirement already satisfied: gymnasium>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (0.29.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.0.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\nDownloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: magent2\n  Building wheel for magent2 (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696098 sha256=921b4c275a909e0d8f16ed7f27d6cfa0237326ef01f829cefaf523731480127f\n  Stored in directory: /tmp/pip-ephem-wheel-cache-z1wsvgxw/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\nSuccessfully built magent2\nInstalling collected packages: pygame, magent2\nSuccessfully installed magent2-0.3.3 pygame-2.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/giangbang/RL-final-project-AIT-3007.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:51:00.684745Z","iopub.execute_input":"2024-12-12T07:51:00.685507Z","iopub.status.idle":"2024-12-12T07:51:02.620820Z","shell.execute_reply.started":"2024-12-12T07:51:00.685476Z","shell.execute_reply":"2024-12-12T07:51:02.619902Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'RL-final-project-AIT-3007'...\nremote: Enumerating objects: 47, done.\u001b[K\nremote: Counting objects: 100% (15/15), done.\u001b[K\nremote: Compressing objects: 100% (9/9), done.\u001b[K\nremote: Total 47 (delta 11), reused 6 (delta 6), pack-reused 32 (from 1)\u001b[K\nReceiving objects: 100% (47/47), 13.67 MiB | 41.55 MiB/s, done.\nResolving deltas: 100% (22/22), done.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/RL-final-project-AIT-3007')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:51:02.622674Z","iopub.execute_input":"2024-12-12T07:51:02.622972Z","iopub.status.idle":"2024-12-12T07:51:02.627338Z","shell.execute_reply.started":"2024-12-12T07:51:02.622943Z","shell.execute_reply":"2024-12-12T07:51:02.626537Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport random\nfrom collections import deque, Counter\nimport os\nfrom magent2.environments import battle_v4\nimport time\n# from torch_model import QNetwork\n\nclass MyQNetwork(nn.Module):\n    def __init__(self, observation_shape, action_shape):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(observation_shape[-1], 13, kernel_size=3),\n            nn.ReLU(),\n            nn.Conv2d(13, 13, kernel_size=3),\n            nn.ReLU()\n        )\n        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n        dummy_output = self.cnn(dummy_input)\n        flatten_dim = dummy_output.view(-1).shape[0]\n        self.fc = nn.Sequential(\n            nn.Linear(flatten_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_shape)\n        )\n\n    def forward(self, x):\n        assert len(x.shape) >= 3, \"only support magent input observation\"\n        x = self.cnn(x)\n        if len(x.shape) == 3:\n            batchsize = 1\n        else:\n            batchsize = x.shape[0]\n        x = x.reshape(batchsize, -1)\n        return self.fc(x)\n\nclass ReplayBuffer(Dataset):\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.buffer = deque(maxlen=capacity)\n        \n    def add(self, state, action, reward, next_state, done):\n        experience = (state, action, reward, next_state, done)\n        self.buffer.append(experience)\n\n    def __len__(self):\n        return len(self.buffer)\n\n    def __getitem__(self, index):\n        return self.buffer[index]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:50:11.576379Z","iopub.execute_input":"2024-12-12T06:50:11.577098Z","iopub.status.idle":"2024-12-12T06:50:15.030753Z","shell.execute_reply.started":"2024-12-12T06:50:11.577053Z","shell.execute_reply":"2024-12-12T06:50:15.030021Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, env, input_shape, action_shape, learning_rate=1e-3):\n        self.env = env\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.q_network = MyQNetwork(input_shape, action_shape).to(self.device)\n        # self.q_network.load_state_dict(\n        #     torch.load(\"/kaggle/input/blue-improved/blue_improved.pt\", weights_only=True)\n        # )\n        self.target_network = MyQNetwork(input_shape, action_shape).to(self.device)\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n        # self.red_pretrained_network = QNetwork(input_shape, action_shape).to(self.device)\n        # self.red_pretrained_network.load_state_dict(\n        #     torch.load(\"/kaggle/working/RL-final-project-AIT-3007/red.pt\", weights_only=True)\n        # )\n\n        self.criterion = nn.MSELoss()\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n        self.replay_buffer = ReplayBuffer(capacity=16200 * 10)\n\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.9\n        self.update_target_every = 2\n\n    def select_action(self, observation, agent):\n        if np.random.rand() <= self.epsilon:\n            return self.env.action_space(agent).sample()\n\n        observation = (\n            torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n        )\n        with torch.inference_mode():\n            q_values = self.q_network(observation)\n        return torch.argmax(q_values, dim=1).item()\n\n    def pretrained_action(self, observation):\n        observation = (\n            torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n        )\n        with torch.inference_mode():\n            q_values = self.red_pretrained_network(observation)\n        return torch.argmax(q_values, dim=1).item()\n\n    def training(self, episodes=50, batch_size=2 ** 10):        \n        for episode in range(episodes):\n            self.env.reset()\n            \n            total_reward = 0\n            reward_for_agent = {agent: 0 for agent in self.env.agents if agent.startswith('blue')}\n            prev_observation = {}\n            prev_action = {}\n            self.env.reset()\n            step = 0\n\n            for idx, agent in enumerate(self.env.agent_iter()):\n                step += 1\n                observation, reward, termination, truncation, info = self.env.last()\n                observation = np.transpose(observation, (2, 0, 1))\n                \n                agent_handle = agent.split('_')[0]\n                \n                if agent_handle == 'blue':\n                    total_reward += reward\n                    reward_for_agent[agent] += reward\n                    \n                if termination or truncation:\n                    action = None\n                else:\n                    if agent_handle == 'blue':\n                        action = self.select_action(observation, agent)\n                    else:\n                        action = self.env.action_space(agent).sample()\n                        # action = self.pretrained_action(observation)\n\n                if agent_handle == 'blue':\n                    prev_observation[agent] = observation\n                    prev_action[agent] = action\n                \n                self.env.step(action)\n                \n                if (idx + 1) % self.env.num_agents == 0:\n                    break\n                \n            for agent in self.env.agent_iter():\n                step += 1\n                \n                observation, reward, termination, truncation, info = self.env.last()\n                observation = np.transpose(observation, (2, 0, 1))\n                \n                agent_handle = agent.split('_')[0]\n                \n                if agent_handle == 'blue':\n                    total_reward += reward\n                    reward_for_agent[agent] += reward\n                    \n                if termination or truncation:\n                    action = None\n                else:\n                    if agent_handle == 'blue':\n                        action = self.select_action(observation, agent)\n                    else:\n                        action = self.env.action_space(agent).sample()\n                        # action = self.pretrained_action(observation)\n    \n                    if agent_handle == 'blue':\n                        self.replay_buffer.add(\n                            prev_observation[agent],\n                            prev_action[agent],\n                            reward,  \n                            observation,\n                            termination\n                        )\n\n                        prev_observation[agent] = observation\n                        prev_action[agent] = action\n    \n                self.env.step(action)\n            \n            dataloader = DataLoader(self.replay_buffer, batch_size=batch_size, shuffle=True)\n            self.update_model(dataloader)\n                \n            if (episode + 1) % self.update_target_every == 0:\n                self.target_network.load_state_dict(self.q_network.state_dict())\n    \n            max_reward = max(reward_for_agent.values())\n            \n            print(f\"Episode {episode}, Epsilon: {self.epsilon:.2f}, Total Reward: {total_reward}, Steps: {step}, Max Reward: {max_reward} \")\n            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n\n    def update_model(self, dataloader):\n        for states, actions, rewards, next_states, dones in dataloader:\n\n            states = torch.tensor(states, dtype=torch.float32).to(self.device)\n            actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n            rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n            next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n            dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n\n            current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n            with torch.inference_mode():\n                next_q_values = self.target_network(next_states).max(1)[0]\n            expected_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n\n            loss = self.criterion(current_q_values, expected_q_values)\n\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:00:36.569990Z","iopub.execute_input":"2024-12-12T07:00:36.570356Z","iopub.status.idle":"2024-12-12T07:00:36.589113Z","shell.execute_reply.started":"2024-12-12T07:00:36.570325Z","shell.execute_reply":"2024-12-12T07:00:36.588155Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"env = battle_v4.env(map_size=45, render_mode=None)\ntrainer = Trainer(env, env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n)\ntrainer.training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:00:40.641164Z","iopub.execute_input":"2024-12-12T07:00:40.641845Z","iopub.status.idle":"2024-12-12T07:12:53.854006Z","shell.execute_reply.started":"2024-12-12T07:00:40.641812Z","shell.execute_reply":"2024-12-12T07:12:53.853051Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/921956130.py:136: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  states = torch.tensor(states, dtype=torch.float32).to(self.device)\n/tmp/ipykernel_30/921956130.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n/tmp/ipykernel_30/921956130.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n/tmp/ipykernel_30/921956130.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n/tmp/ipykernel_30/921956130.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n","output_type":"stream"},{"name":"stdout","text":"Episode 0, Epsilon: 1.00, Total Reward: -3244.045118597336, Steps: 159574, Max Reward: -4.495000167749822 \nEpisode 1, Epsilon: 0.90, Total Reward: -3051.900108466856, Steps: 158424, Max Reward: -31.700001289136708 \nEpisode 2, Epsilon: 0.81, Total Reward: -2698.4000980220735, Steps: 149892, Max Reward: -21.500001321546733 \nEpisode 3, Epsilon: 0.73, Total Reward: -2163.8750888127834, Steps: 105450, Max Reward: 25.3999986872077 \nEpisode 4, Epsilon: 0.66, Total Reward: -2322.300077858381, Steps: 155153, Max Reward: -20.700001030229032 \nEpisode 5, Epsilon: 0.59, Total Reward: -1984.8000696692616, Steps: 133078, Max Reward: -10.400000834837556 \nEpisode 6, Epsilon: 0.53, Total Reward: -1622.09005906526, Steps: 117556, Max Reward: 1.9999992102384567 \nEpisode 7, Epsilon: 0.48, Total Reward: -1786.0000558216125, Steps: 153216, Max Reward: -6.200000794604421 \nEpisode 8, Epsilon: 0.43, Total Reward: -1502.6000496596098, Steps: 139066, Max Reward: 22.299999219365418 \nEpisode 9, Epsilon: 0.39, Total Reward: 317.8199946824461, Steps: 9384, Max Reward: 24.739999854005873 \nEpisode 10, Epsilon: 0.35, Total Reward: 34.2799875959754, Steps: 25511, Max Reward: 27.47999973781407 \nEpisode 11, Epsilon: 0.31, Total Reward: -311.1900199530646, Steps: 47793, Max Reward: 16.609999713487923 \nEpisode 12, Epsilon: 0.28, Total Reward: -881.4200298599899, Steps: 104775, Max Reward: 90.79999917559326 \nEpisode 13, Epsilon: 0.25, Total Reward: 325.9649949017912, Steps: 8901, Max Reward: 24.764999847859144 \nEpisode 14, Epsilon: 0.23, Total Reward: -689.3000234449282, Steps: 86438, Max Reward: 12.199999636039138 \nEpisode 15, Epsilon: 0.21, Total Reward: -657.7000208338723, Steps: 93551, Max Reward: 20.29999950248748 \nEpisode 16, Epsilon: 0.19, Total Reward: 35.15499093942344, Steps: 35474, Max Reward: 31.254999733529985 \nEpisode 17, Epsilon: 0.17, Total Reward: -525.7250163946301, Steps: 90128, Max Reward: 42.79999936558306 \nEpisode 18, Epsilon: 0.15, Total Reward: 314.67499589920044, Steps: 14973, Max Reward: 45.87499971501529 \nEpisode 19, Epsilon: 0.14, Total Reward: 325.2349960850552, Steps: 13115, Max Reward: 34.534999806433916 \nEpisode 20, Epsilon: 0.12, Total Reward: 377.1449966831133, Steps: 6734, Max Reward: 25.24499987438321 \nEpisode 21, Epsilon: 0.11, Total Reward: 377.2299968926236, Steps: 7761, Max Reward: 25.00499988347292 \nEpisode 22, Epsilon: 0.10, Total Reward: -304.8050075350329, Steps: 83464, Max Reward: 17.59999981150031 \nEpisode 23, Epsilon: 0.09, Total Reward: 352.4049958344549, Steps: 7494, Max Reward: 23.804999854415655 \nEpisode 24, Epsilon: 0.08, Total Reward: 370.60499661415815, Steps: 7274, Max Reward: 25.104999876581132 \nEpisode 25, Epsilon: 0.07, Total Reward: 377.0149967279285, Steps: 6878, Max Reward: 29.934999869205058 \nEpisode 26, Epsilon: 0.06, Total Reward: 352.8699960336089, Steps: 8102, Max Reward: 24.569999878294766 \nEpisode 27, Epsilon: 0.06, Total Reward: 323.41999609209597, Steps: 12234, Max Reward: 24.3199998838827 \nEpisode 28, Epsilon: 0.05, Total Reward: 380.1449969848618, Steps: 7249, Max Reward: 24.109999867156148 \nEpisode 29, Epsilon: 0.05, Total Reward: 380.43499686568975, Steps: 6897, Max Reward: 24.634999876841903 \nEpisode 30, Epsilon: 0.04, Total Reward: 371.0349966632202, Steps: 7550, Max Reward: 24.094999901019037 \nEpisode 31, Epsilon: 0.04, Total Reward: -702.6500246403739, Steps: 83659, Max Reward: 11.199999867007136 \nEpisode 32, Epsilon: 0.03, Total Reward: 373.93499659281224, Steps: 7091, Max Reward: 24.03499986231327 \nEpisode 33, Epsilon: 0.03, Total Reward: 384.08499729726464, Steps: 7852, Max Reward: 24.584999883547425 \nEpisode 34, Epsilon: 0.03, Total Reward: 378.4849969698116, Steps: 7757, Max Reward: 19.599999903701246 \nEpisode 35, Epsilon: 0.03, Total Reward: 378.19499749783427, Steps: 9697, Max Reward: 19.79499990772456 \nEpisode 36, Epsilon: 0.02, Total Reward: 389.67999741993845, Steps: 7948, Max Reward: 24.804999891668558 \nEpisode 37, Epsilon: 0.02, Total Reward: 383.1349969925359, Steps: 7021, Max Reward: 15.7349999146536 \nEpisode 38, Epsilon: 0.02, Total Reward: 7.485000276938081, Steps: 67737, Max Reward: 19.06499989517033 \nEpisode 39, Epsilon: 0.02, Total Reward: 374.0499968705699, Steps: 8403, Max Reward: 24.17999987769872 \nEpisode 40, Epsilon: 0.01, Total Reward: 330.8949975762516, Steps: 18027, Max Reward: 19.734999901615083 \nEpisode 41, Epsilon: 0.01, Total Reward: 369.0899971174076, Steps: 10280, Max Reward: 24.489999852143228 \nEpisode 42, Epsilon: 0.01, Total Reward: 351.60499758273363, Steps: 16173, Max Reward: 29.329999780282378 \nEpisode 43, Epsilon: 0.01, Total Reward: 368.2499971622601, Steps: 9919, Max Reward: 29.47499985434115 \nEpisode 44, Epsilon: 0.01, Total Reward: 360.0499970847741, Steps: 11123, Max Reward: 29.61499986425042 \nEpisode 45, Epsilon: 0.01, Total Reward: 345.55499784462154, Steps: 15802, Max Reward: 19.354999916628003 \nEpisode 46, Epsilon: 0.01, Total Reward: 330.69999766815454, Steps: 17679, Max Reward: 28.76499982830137 \nEpisode 47, Epsilon: 0.01, Total Reward: 346.2549979882315, Steps: 16825, Max Reward: 24.90499989502132 \nEpisode 48, Epsilon: 0.01, Total Reward: 327.9499981282279, Steps: 19772, Max Reward: 18.449999902397394 \nEpisode 49, Epsilon: 0.01, Total Reward: 365.67999721877277, Steps: 11113, Max Reward: 24.44499988667667 \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"os.makedirs(\"models\", exist_ok=True)\ntorch.save(trainer.q_network.state_dict(), \"models/blue_vs_random.pt\")\nprint(\"Training complete. Model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:12:56.841403Z","iopub.execute_input":"2024-12-12T07:12:56.842120Z","iopub.status.idle":"2024-12-12T07:12:56.851435Z","shell.execute_reply.started":"2024-12-12T07:12:56.842086Z","shell.execute_reply":"2024-12-12T07:12:56.850583Z"}},"outputs":[{"name":"stdout","text":"Training complete. Model saved.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# make video\nimport cv2\n\nenv = battle_v4.env(map_size=45, render_mode=\"rgb_array\", max_cycles=300)\nvid_dir = \"video\"\nos.makedirs(vid_dir, exist_ok=True)\nfps = 35\nframes = []\nmy_q_network = MyQNetwork(\n    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n)\nmy_q_network.load_state_dict(\n    torch.load(\"/kaggle/working/models/blue_vs_random.pt\", weights_only=True, map_location=\"cpu\")\n)\nmy_q_network.eval()\n\nred_pretrained_network = QNetwork(\n    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n)\nred_pretrained_network.load_state_dict(\n    torch.load(\"/kaggle/working/RL-final-project-AIT-3007/red.pt\", weights_only=True, map_location=\"cpu\")\n)\nenv.reset()\nfor agent in env.agent_iter():\n\n    observation, reward, termination, truncation, info = env.last()\n    observation = (\n        torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0)\n    )\n    if termination or truncation:\n        action = None  # this agent has died\n    else:\n        agent_handle = agent.split(\"_\")[0]\n        if agent_handle == \"blue\":\n            with torch.inference_mode():\n                q_values = my_q_network(observation)\n            action = torch.argmax(q_values, dim=1).numpy()[0]\n        else:\n            action = env.action_space(agent).sample()\n            # with torch.inference_mode():\n            #     q_values = red_pretrained_network(observation)\n            # action = torch.argmax(q_values, dim=1).item()\n            \n    env.step(action)\n\n    if agent == \"red_0\":\n        frames.append(env.render())\n\nheight, width, _ = frames[0].shape\nout = cv2.VideoWriter(\n    os.path.join(vid_dir, f\"blue_vs_random_another.mp4\"),\n    cv2.VideoWriter_fourcc(*\"mp4v\"),\n    fps,\n    (width, height),\n)\nfor frame in frames:\n    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n    out.write(frame_bgr)\nout.release()\nprint(\"Done recording pretrained agents\")\n\nenv.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:13:25.023410Z","iopub.execute_input":"2024-12-12T07:13:25.024247Z","iopub.status.idle":"2024-12-12T07:13:36.154559Z","shell.execute_reply.started":"2024-12-12T07:13:25.024212Z","shell.execute_reply":"2024-12-12T07:13:36.153925Z"}},"outputs":[{"name":"stdout","text":"Done recording pretrained agents\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class vQNetwork(nn.Module):\n    def __init__(self, observation_shape, action_shape):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n            nn.ReLU(),\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n            nn.ReLU(),\n        )\n        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n        dummy_output = self.cnn(dummy_input)\n        flatten_dim = dummy_output.view(-1).shape[0]\n        self.network = nn.Sequential(\n            nn.Linear(flatten_dim, 120),\n            nn.ReLU(),\n            nn.Linear(120, 84),\n            nn.ReLU(),\n            nn.Linear(84, action_shape),\n        )\n\n    def forward(self, x):\n        assert len(x.shape) >= 3, \"only support magent input observation\"\n        x = self.cnn(x)\n        if len(x.shape) == 3:\n            batchsize = 1\n        else:\n            batchsize = x.shape[0]\n        x = x.reshape(batchsize, -1)\n        return self.network(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:53:42.610651Z","iopub.execute_input":"2024-12-12T07:53:42.611455Z","iopub.status.idle":"2024-12-12T07:53:42.618573Z","shell.execute_reply.started":"2024-12-12T07:53:42.611420Z","shell.execute_reply":"2024-12-12T07:53:42.617616Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# from torch_model import QNetwork\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    tqdm = lambda x, *args, **kwargs: x  # Fallback: tqdm becomes a no-op\n    \ndef eval():\n    max_cycles = 300\n    env = battle_v4.env(map_size=45, max_cycles=max_cycles)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    def random_policy(env, agent, obs):\n        return env.action_space(agent).sample()\n\n    MyNetwork = MyQNetwork(\n        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n    )\n    MyNetwork.load_state_dict(\n        torch.load(\"/kaggle/input/blue-improved/blue_improved.pt\", weights_only=True, map_location='cpu')\n    )\n    MyNetwork.to(device)\n\n    red_pretrained_network = vQNetwork(\n        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n    )\n    red_pretrained_network.load_state_dict(\n        torch.load(\"/kaggle/working/RL-final-project-AIT-3007/red.pt\", weights_only=True, map_location=\"cpu\")\n    )\n    red_pretrained_network.to(device)\n    \n    v1 = vQNetwork(\n        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n    )\n    v1.load_state_dict(\n        torch.load(\"/kaggle/input/another-model/v1.pth\", weights_only=True, map_location=\"cpu\")\n    )\n    v1.to(device)\n\n    v2 = vQNetwork(\n        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n    )\n    v2.load_state_dict(\n        torch.load(\"/kaggle/input/another-model/v2.pth\", weights_only=True, map_location=\"cpu\")\n    )\n    v2.to(device)\n\n    def pretrain_policy(env, agent, obs):\n        observation = (\n            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n        )\n        with torch.inference_mode():\n            q_values = red_pretrained_network(observation)\n        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n        \n    def v1_policy(env, agent, obs):\n        observation = (\n            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n        )\n        with torch.inference_mode():\n            q_values = v1(observation)\n        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n\n    def v2_policy(env, agent, obs):\n        observation = (\n            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n        )\n        with torch.inference_mode():\n            q_values = v2(observation)\n        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n\n    def my_policy(env, agent, obs):\n        observation = (\n            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n        )\n        with torch.inference_mode():\n            q_values = MyNetwork(observation)\n        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n\n    def run_eval(env, red_policy, blue_policy, n_episode: int = 100):\n        red_win, blue_win = [], []\n        red_tot_rw, blue_tot_rw = [], []\n        n_agent_each_team = len(env.env.action_spaces) // 2\n\n        for _ in tqdm(range(n_episode)):\n            env.reset()\n            n_kill = {\"red\": 0, \"blue\": 0}\n            red_reward, blue_reward = 0, 0\n\n            for agent in env.agent_iter():\n                observation, reward, termination, truncation, info = env.last()\n                agent_team = agent.split(\"_\")[0]\n\n                n_kill[agent_team] += (\n                    reward > 4.5\n                )  # This assumes default reward settups\n                if agent_team == \"red\":\n                    red_reward += reward\n                else:\n                    blue_reward += reward\n\n                if termination or truncation:\n                    action = None  # this agent has died\n                else:\n                    if agent_team == \"red\":\n                        action = red_policy(env, agent, observation)\n                    else:\n                        action = blue_policy(env, agent, observation)\n\n                env.step(action)\n\n            who_wins = \"red\" if n_kill[\"red\"] >= n_kill[\"blue\"] + 5 else \"draw\"\n            who_wins = \"blue\" if n_kill[\"red\"] + 5 <= n_kill[\"blue\"] else who_wins\n            red_win.append(who_wins == \"red\")\n            blue_win.append(who_wins == \"blue\")\n\n            red_tot_rw.append(red_reward / n_agent_each_team)\n            blue_tot_rw.append(blue_reward / n_agent_each_team)\n\n        return {\n            \"winrate_red\": np.mean(red_win),\n            \"winrate_blue\": np.mean(blue_win),\n            \"average_rewards_red\": np.mean(red_tot_rw),\n            \"average_rewards_blue\": np.mean(blue_tot_rw),\n        }\n\n    print(\"=\" * 20)\n    print(\"Eval with pretrain policy\")\n    print(\n        run_eval(\n            env=env, red_policy=pretrain_policy, blue_policy=my_policy, n_episode=30\n        )\n    )\n\n    print(\"=\" * 20)\n    print(\"Eval with v1 policy\")\n    print(\n        run_eval(\n            env=env, red_policy=v1_policy, blue_policy=my_policy, n_episode=30\n        )\n    )\n    print(\"=\" * 20)\n\n    print(\"Eval with v2 policy\")\n    print(\n        run_eval(\n            env=env, red_policy=v2_policy, blue_policy=my_policy, n_episode=30\n        )\n    )\n    print(\"=\" * 20)\n\nif __name__ == \"__main__\":\n    eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:57:31.384664Z","iopub.execute_input":"2024-12-12T07:57:31.385371Z","iopub.status.idle":"2024-12-12T08:04:53.006239Z","shell.execute_reply.started":"2024-12-12T07:57:31.385333Z","shell.execute_reply":"2024-12-12T08:04:53.005407Z"}},"outputs":[{"name":"stdout","text":"====================\nEval with pretrain policy\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30/30 [01:46<00:00,  3.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"{'winrate_red': 0.0, 'winrate_blue': 1.0, 'average_rewards_red': 0.7385699528127663, 'average_rewards_blue': 4.133952614933313}\n====================\nEval with v1 policy\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30/30 [03:21<00:00,  6.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"{'winrate_red': 0.0, 'winrate_blue': 0.9333333333333333, 'average_rewards_red': 3.603516443757869, 'average_rewards_blue': 2.691331182867519}\n====================\nEval with v2 policy\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30/30 [02:14<00:00,  4.47s/it]","output_type":"stream"},{"name":"stdout","text":"{'winrate_red': 0.4, 'winrate_blue': 0.23333333333333334, 'average_rewards_red': 4.591510265306359, 'average_rewards_blue': 3.4396933533274283}\n====================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":19}]}