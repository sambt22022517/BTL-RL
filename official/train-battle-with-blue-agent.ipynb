{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d929a83",
   "metadata": {
    "papermill": {
     "duration": 0.003023,
     "end_time": "2024-12-16T17:01:23.576037",
     "exception": false,
     "start_time": "2024-12-16T17:01:23.573014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tải các thư viện và repo cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db90c83",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-16T17:01:23.582368Z",
     "iopub.status.busy": "2024-12-16T17:01:23.582049Z",
     "iopub.status.idle": "2024-12-16T17:01:58.707993Z",
     "shell.execute_reply": "2024-12-16T17:01:58.706964Z"
    },
    "papermill": {
     "duration": 35.131658,
     "end_time": "2024-12-16T17:01:58.710179",
     "exception": false,
     "start_time": "2024-12-16T17:01:23.578521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Farama-Foundation/MAgent2\r\n",
      "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-da1gasp_\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-da1gasp_\r\n",
      "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.26.4)\r\n",
      "Collecting pygame>=2.1.0 (from magent2==0.3.3)\r\n",
      "  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: pettingzoo>=1.23.1 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.24.0)\r\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (0.29.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\r\n",
      "Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: magent2\r\n",
      "  Building wheel for magent2 (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696132 sha256=936dba94692749c8a60e4ee4392e580379eb5cf4c58315a506ed326ec9909e32\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fh9kfxy1/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\r\n",
      "Successfully built magent2\r\n",
      "Installing collected packages: pygame, magent2\r\n",
      "Successfully installed magent2-0.3.3 pygame-2.6.1\r\n",
      "Cloning into 'RL-final-project-AIT-3007'...\r\n",
      "remote: Enumerating objects: 56, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (24/24), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\r\n",
      "remote: Total 56 (delta 12), reused 13 (delta 6), pack-reused 32 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (56/56), 17.78 MiB | 34.15 MiB/s, done.\r\n",
      "Resolving deltas: 100% (23/23), done.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/Farama-Foundation/MAgent2\n",
    "!git clone https://github.com/giangbang/RL-final-project-AIT-3007.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651be6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:01:58.721951Z",
     "iopub.status.busy": "2024-12-16T17:01:58.721603Z",
     "iopub.status.idle": "2024-12-16T17:01:58.726017Z",
     "shell.execute_reply": "2024-12-16T17:01:58.725291Z"
    },
    "papermill": {
     "duration": 0.012234,
     "end_time": "2024-12-16T17:01:58.727681",
     "exception": false,
     "start_time": "2024-12-16T17:01:58.715447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/RL-final-project-AIT-3007')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5b553",
   "metadata": {
    "papermill": {
     "duration": 0.004756,
     "end_time": "2024-12-16T17:01:58.737483",
     "exception": false,
     "start_time": "2024-12-16T17:01:58.732727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Định nghĩa mô hình sử dụng:\n",
    "\n",
    "- **Mô hình có 2 phần chính**: *CNN* và *Fully Connected* (FC).\n",
    "\n",
    "### 1. **CNN (Convolutional Neural Network)**:\n",
    "   - Bao gồm **3 lớp `Conv2d`**, mỗi lớp có:\n",
    "     - **Số lượng đầu ra (output channels):** 13.\n",
    "     - **Hàm kích hoạt phi tuyến:** ReLU.\n",
    "\n",
    "### 2. **Fully Connected (FC)**:\n",
    "   - Bao gồm **2 lớp tuyến tính (Linear)**:\n",
    "     - Lớp 1: Đầu ra là **256**.\n",
    "     - Lớp 2: Đầu ra là **128**.\n",
    "   - Sau mỗi lớp tuyến tính là **hàm kích hoạt phi tuyến ReLU**.\n",
    "   - Lớp `Linear` cuối cùng có chức năng **chiếu (project)** các đặc trưng (**features**) xuống không gian có kích thước **128**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb982933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:01:58.749663Z",
     "iopub.status.busy": "2024-12-16T17:01:58.748909Z",
     "iopub.status.idle": "2024-12-16T17:02:02.249963Z",
     "shell.execute_reply": "2024-12-16T17:02:02.249230Z"
    },
    "papermill": {
     "duration": 3.50952,
     "end_time": "2024-12-16T17:02:02.251951",
     "exception": false,
     "start_time": "2024-12-16T17:01:58.742431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, Counter\n",
    "import os\n",
    "from magent2.environments import battle_v4\n",
    "import time\n",
    "\n",
    "class MyQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], 13, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(13, 13, kernel_size=3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(13, 13, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        out = self.cnn(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "            \n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "            \n",
    "        out = out.reshape(batchsize, -1)\n",
    "        \n",
    "        return self.fc(out)\n",
    "\n",
    "class ReplayBuffer(Dataset):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.buffer[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979fc47",
   "metadata": {
    "papermill": {
     "duration": 0.004683,
     "end_time": "2024-12-16T17:02:02.261852",
     "exception": false,
     "start_time": "2024-12-16T17:02:02.257169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phương pháp huấn luyện\n",
    "\n",
    "### **Thuật toán sử dụng**: DQN (Deep Q-Network)\n",
    "- `q_network` được cập nhật sau mỗi lần gọi `dataloader`, sau khi hoàn thành 1 episode.\n",
    "\n",
    "### **Cách huấn luyện**:\n",
    "1. **Thêm dữ liệu vào ReplayBuffer**:\n",
    "   - Sau mỗi episode, một chuỗi các *trajectories* sẽ được lưu vào `ReplayBuffer`.\n",
    "2. **Cập nhật `q_network`**:\n",
    "   - Sau đó hàm `update_model` được gọi để cập nhật các tham số của `q_network`.\n",
    "\n",
    "### **Các tham số**:\n",
    "- **`steplr`**:\n",
    "  - Learning rate được giảm sau mỗi lần `update_every_target` với tỉ lệ:\n",
    "    - `gamma = 0.9`.\n",
    "- **Discount factor**: `0.9`.\n",
    "- **Epsilon (`\\epsilon`) trong \\(\\epsilon\\)-greedy**:\n",
    "  - Giảm theo tỷ lệ `0.96` mỗi bước.\n",
    "  - Tối thiểu giảm xuống còn `0.1`.\n",
    "- **Cập nhật mô hình `target`**:\n",
    "  - Mô hình `target` sẽ được cập nhật sau mỗi `update_every_target = 2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed0665c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:02:02.273203Z",
     "iopub.status.busy": "2024-12-16T17:02:02.272833Z",
     "iopub.status.idle": "2024-12-16T17:02:02.290457Z",
     "shell.execute_reply": "2024-12-16T17:02:02.289814Z"
    },
    "papermill": {
     "duration": 0.025202,
     "end_time": "2024-12-16T17:02:02.291994",
     "exception": false,
     "start_time": "2024-12-16T17:02:02.266792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "class Trainer:\n",
    "    def __init__(self, env, config_qnet=None, input_shape=None, action_shape=None, learning_rate=1e-3):\n",
    "        self.env = env\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_network = MyQNetwork(input_shape, action_shape).to(self.device)\n",
    "        \n",
    "        self.target_network = MyQNetwork(input_shape, action_shape).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.steplr = lr_scheduler.StepLR(optimizer=self.optimizer, step_size=1, gamma=0.9)\n",
    "        self.replay_buffer = ReplayBuffer(capacity=16200 * 10)\n",
    "\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.96\n",
    "        self.update_target_every = 2\n",
    "\n",
    "    def select_action(self, observation, agent):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self.env.action_space(agent).sample()\n",
    "\n",
    "        observation = (\n",
    "            torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n",
    "        )\n",
    "        self.q_network.eval()\n",
    "        with torch.inference_mode():\n",
    "            q_values = self.q_network(observation)\n",
    "            # print(q_values)\n",
    "        return torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "    # def pretrained_action(self, observation):\n",
    "    #     observation = (\n",
    "    #         torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n",
    "    #     )\n",
    "    #     self.red_pretrained_network.eval()\n",
    "    #     with torch.inference_mode():\n",
    "    #         q_values = self.red_pretrained_network(observation)\n",
    "    #     return torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "    def training(self, episodes=100, batch_size=2 ** 12):        \n",
    "        for episode in range(episodes):\n",
    "            self.env.reset()\n",
    "            \n",
    "            total_reward = 0\n",
    "            reward_for_agent = {agent: 0 for agent in self.env.agents if agent.startswith('blue')}\n",
    "            prev_observation = {}\n",
    "            prev_action = {}\n",
    "            self.env.reset()\n",
    "            step = 0\n",
    "\n",
    "            for idx, agent in enumerate(self.env.agent_iter()):\n",
    "                step += 1\n",
    "                observation, reward, termination, truncation, info = self.env.last()\n",
    "                observation = np.transpose(observation, (2, 0, 1))\n",
    "                \n",
    "                agent_handle = agent.split('_')[0]\n",
    "                \n",
    "                if agent_handle == 'blue':\n",
    "                    total_reward += reward\n",
    "                    reward_for_agent[agent] += reward\n",
    "                    \n",
    "                if termination or truncation:\n",
    "                    action = None\n",
    "                else:\n",
    "                    if agent_handle == 'blue':\n",
    "                        action = self.select_action(observation, agent)\n",
    "                    else:\n",
    "                        action = self.env.action_space(agent).sample()\n",
    "                        # action = self.pretrained_action(observation)\n",
    "\n",
    "                if agent_handle == 'blue':\n",
    "                    prev_observation[agent] = observation\n",
    "                    prev_action[agent] = action\n",
    "                \n",
    "                self.env.step(action)\n",
    "                \n",
    "                if (idx + 1) % self.env.num_agents == 0:\n",
    "                    break\n",
    "                \n",
    "            for agent in self.env.agent_iter():\n",
    "                step += 1\n",
    "                \n",
    "                observation, reward, termination, truncation, info = self.env.last()\n",
    "                observation = np.transpose(observation, (2, 0, 1))\n",
    "                \n",
    "                agent_handle = agent.split('_')[0]\n",
    "                \n",
    "                if agent_handle == 'blue':\n",
    "                    total_reward += reward\n",
    "                    reward_for_agent[agent] += reward\n",
    "                    \n",
    "                if termination or truncation:\n",
    "                    action = None\n",
    "                else:\n",
    "                    if agent_handle == 'blue':\n",
    "                        action = self.select_action(observation, agent)\n",
    "                    else:\n",
    "                        action = self.env.action_space(agent).sample()\n",
    "                        # action = self.pretrained_action(observation)\n",
    "    \n",
    "                    if agent_handle == 'blue':\n",
    "                        self.replay_buffer.add(\n",
    "                            prev_observation[agent],\n",
    "                            prev_action[agent],\n",
    "                            reward,  \n",
    "                            observation,\n",
    "                            termination\n",
    "                        )\n",
    "\n",
    "                        prev_observation[agent] = observation\n",
    "                        prev_action[agent] = action\n",
    "    \n",
    "                self.env.step(action)\n",
    "            \n",
    "            dataloader = DataLoader(self.replay_buffer, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "            self.update_model(dataloader)\n",
    "                \n",
    "            if (episode + 1) % self.update_target_every == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "                self.steplr.step()\n",
    "    \n",
    "            max_reward = max(reward_for_agent.values())\n",
    "            \n",
    "            print(f\"Episode {episode}, Epsilon: {self.epsilon:.2f}, Total Reward: {total_reward}, Steps: {step}, Max Reward: {max_reward}, lr: {self.steplr.get_last_lr()} \")\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def update_model(self, dataloader):\n",
    "        self.q_network.train()\n",
    "        for states, actions, rewards, next_states, dones in dataloader:\n",
    "            # print(states.shape)\n",
    "\n",
    "            states = states.to(dtype=torch.float32, device=self.device)\n",
    "            actions = actions.to(dtype=torch.long, device=self.device)\n",
    "            rewards = rewards.to(dtype=torch.float32, device=self.device)\n",
    "            next_states = next_states.to(dtype=torch.float32, device=self.device)\n",
    "            dones = dones.to(dtype=torch.float32, device=self.device)\n",
    "\n",
    "            current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            with torch.inference_mode():\n",
    "                next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            expected_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "            loss = self.criterion(current_q_values, expected_q_values)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234e3ac",
   "metadata": {
    "papermill": {
     "duration": 0.004638,
     "end_time": "2024-12-16T17:02:02.301464",
     "exception": false,
     "start_time": "2024-12-16T17:02:02.296826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bắt đầu huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e724b61e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:02:02.311957Z",
     "iopub.status.busy": "2024-12-16T17:02:02.311736Z",
     "iopub.status.idle": "2024-12-16T17:14:38.324551Z",
     "shell.execute_reply": "2024-12-16T17:14:38.323310Z"
    },
    "papermill": {
     "duration": 756.020431,
     "end_time": "2024-12-16T17:14:38.326640",
     "exception": false,
     "start_time": "2024-12-16T17:02:02.306209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Epsilon: 1.00, Total Reward: -3283.7001208886504, Steps: 159056, Max Reward: -32.00000128429383, lr: [0.001] \n",
      "Episode 1, Epsilon: 0.96, Total Reward: -3166.280114626512, Steps: 158571, Max Reward: -9.060000314377248, lr: [0.0009000000000000001] \n",
      "Episode 2, Epsilon: 0.92, Total Reward: -2992.7851090747863, Steps: 156817, Max Reward: -11.125000424683094, lr: [0.0009000000000000001] \n",
      "Episode 3, Epsilon: 0.88, Total Reward: -2891.8201054576784, Steps: 156284, Max Reward: -18.520000678487122, lr: [0.0008100000000000001] \n",
      "Episode 4, Epsilon: 0.85, Total Reward: -2851.5701018059626, Steps: 156379, Max Reward: -19.400001253932714, lr: [0.0008100000000000001] \n",
      "Episode 5, Epsilon: 0.82, Total Reward: -2599.220096149482, Steps: 149153, Max Reward: -0.0700002321973443, lr: [0.000729] \n",
      "Episode 6, Epsilon: 0.78, Total Reward: -2719.7000952856615, Steps: 155466, Max Reward: -24.90000123810023, lr: [0.000729] \n",
      "Episode 7, Epsilon: 0.75, Total Reward: -2440.2800892386585, Steps: 141278, Max Reward: -4.880000252276659, lr: [0.0006561000000000001] \n",
      "Episode 8, Epsilon: 0.72, Total Reward: -2232.9450852386653, Steps: 123254, Max Reward: 11.044999712146819, lr: [0.0006561000000000001] \n",
      "Episode 9, Epsilon: 0.69, Total Reward: -2345.700085057877, Steps: 143873, Max Reward: 15.39999881759286, lr: [0.00059049] \n",
      "Episode 10, Epsilon: 0.66, Total Reward: -2059.600079735741, Steps: 112335, Max Reward: 33.29999871272594, lr: [0.00059049] \n",
      "Episode 11, Epsilon: 0.64, Total Reward: -1873.230075306259, Steps: 92078, Max Reward: 4.099999031983316, lr: [0.000531441] \n",
      "Episode 12, Epsilon: 0.61, Total Reward: 43.45498541928828, Steps: 20954, Max Reward: 21.354999746195972, lr: [0.000531441] \n",
      "Episode 13, Epsilon: 0.59, Total Reward: -1786.4000701662153, Steps: 94407, Max Reward: 9.199998999014497, lr: [0.0004782969] \n",
      "Episode 14, Epsilon: 0.56, Total Reward: -55.1800173940137, Steps: 25402, Max Reward: 24.86499963607639, lr: [0.0004782969] \n",
      "Episode 15, Epsilon: 0.54, Total Reward: -1317.9550553532317, Steps: 98626, Max Reward: 76.46499892603606, lr: [0.00043046721] \n",
      "Episode 16, Epsilon: 0.52, Total Reward: -1533.8900607684627, Steps: 90400, Max Reward: 14.399999104440212, lr: [0.00043046721] \n",
      "Episode 17, Epsilon: 0.50, Total Reward: 184.47499034367502, Steps: 15209, Max Reward: 18.474999809637666, lr: [0.000387420489] \n",
      "Episode 18, Epsilon: 0.48, Total Reward: -492.7150293122977, Steps: 51442, Max Reward: 44.009999421425164, lr: [0.000387420489] \n",
      "Episode 19, Epsilon: 0.46, Total Reward: 287.6149935172871, Steps: 10237, Max Reward: 18.9149998575449, lr: [0.0003486784401] \n",
      "Episode 20, Epsilon: 0.44, Total Reward: 280.219993179664, Steps: 9714, Max Reward: 19.21999984793365, lr: [0.0003486784401] \n",
      "Episode 21, Epsilon: 0.42, Total Reward: 298.69999382738024, Steps: 9478, Max Reward: 19.5449998434633, lr: [0.00031381059609000004] \n",
      "Episode 22, Epsilon: 0.41, Total Reward: 240.73499243706465, Steps: 12608, Max Reward: 23.134999826550484, lr: [0.00031381059609000004] \n",
      "Episode 23, Epsilon: 0.39, Total Reward: 314.7449945323169, Steps: 8831, Max Reward: 24.14499985612929, lr: [0.00028242953648100003] \n",
      "Episode 24, Epsilon: 0.38, Total Reward: 305.2999943709001, Steps: 9403, Max Reward: 28.099999816156924, lr: [0.00028242953648100003] \n",
      "Episode 25, Epsilon: 0.36, Total Reward: 318.5649945959449, Steps: 8440, Max Reward: 24.764999859035015, lr: [0.00025418658283290005] \n",
      "Episode 26, Epsilon: 0.35, Total Reward: 324.14499492384493, Steps: 8943, Max Reward: 20.344999883323908, lr: [0.00025418658283290005] \n",
      "Episode 27, Epsilon: 0.33, Total Reward: 331.06499511469156, Steps: 8440, Max Reward: 19.16499988734722, lr: [0.00022876792454961005] \n",
      "Episode 28, Epsilon: 0.32, Total Reward: 333.1649952158332, Steps: 8349, Max Reward: 33.86499983537942, lr: [0.00022876792454961005] \n",
      "Episode 29, Epsilon: 0.31, Total Reward: 347.32499558571726, Steps: 7544, Max Reward: 29.309999832883477, lr: [0.00020589113209464906] \n",
      "Episode 30, Epsilon: 0.29, Total Reward: 355.8249958753586, Steps: 7161, Max Reward: 29.82499984279275, lr: [0.00020589113209464906] \n",
      "Episode 31, Epsilon: 0.28, Total Reward: 346.309995546937, Steps: 7529, Max Reward: 29.80999984871596, lr: [0.00018530201888518417] \n",
      "Episode 32, Epsilon: 0.27, Total Reward: 355.5349957989529, Steps: 7061, Max Reward: 24.634999876841903, lr: [0.00018530201888518417] \n",
      "Episode 33, Epsilon: 0.26, Total Reward: 363.42999617476016, Steps: 7137, Max Reward: 29.92999984137714, lr: [0.00016677181699666576] \n",
      "Episode 34, Epsilon: 0.25, Total Reward: 364.24999610427767, Steps: 6665, Max Reward: 24.74999986961484, lr: [0.00016677181699666576] \n",
      "Episode 35, Epsilon: 0.24, Total Reward: 359.0199960274622, Steps: 7206, Max Reward: 19.819999900646508, lr: [0.0001500946352969992] \n",
      "Episode 36, Epsilon: 0.23, Total Reward: 363.7499960772693, Steps: 6651, Max Reward: 24.349999863654375, lr: [0.0001500946352969992] \n",
      "Episode 37, Epsilon: 0.22, Total Reward: 367.25499620102346, Steps: 6561, Max Reward: 25.254999862983823, lr: [0.0001350851717672993] \n",
      "Episode 38, Epsilon: 0.21, Total Reward: 374.9699964579195, Steps: 6218, Max Reward: 24.169999865815043, lr: [0.0001350851717672993] \n",
      "Episode 39, Epsilon: 0.20, Total Reward: 363.3399961180985, Steps: 6757, Max Reward: 25.23999988567084, lr: [0.00012157665459056936] \n",
      "Episode 40, Epsilon: 0.20, Total Reward: 370.75999631918967, Steps: 6457, Max Reward: 29.65999984461814, lr: [0.00012157665459056936] \n",
      "Episode 41, Epsilon: 0.19, Total Reward: 374.24999653268605, Steps: 6645, Max Reward: 24.549999866634607, lr: [0.00010941898913151243] \n",
      "Episode 42, Epsilon: 0.18, Total Reward: 371.0549963694066, Steps: 6423, Max Reward: 19.954999898560345, lr: [0.00010941898913151243] \n",
      "Episode 43, Epsilon: 0.17, Total Reward: 371.42999651748687, Steps: 6888, Max Reward: 25.129999864846468, lr: [9.847709021836118e-05] \n",
      "Episode 44, Epsilon: 0.17, Total Reward: 376.74999663978815, Steps: 6503, Max Reward: 24.049999861977994, lr: [9.847709021836118e-05] \n",
      "Episode 45, Epsilon: 0.16, Total Reward: 371.4449964389205, Steps: 6595, Max Reward: 19.74499988462776, lr: [8.862938119652506e-05] \n",
      "Episode 46, Epsilon: 0.15, Total Reward: 371.64999641571194, Steps: 6517, Max Reward: 25.64999987464398, lr: [8.862938119652506e-05] \n",
      "Episode 47, Epsilon: 0.15, Total Reward: 379.0549967121333, Steps: 6474, Max Reward: 20.25499989464879, lr: [7.976644307687256e-05] \n",
      "Episode 48, Epsilon: 0.14, Total Reward: 376.96499656978995, Steps: 6190, Max Reward: 24.964999883435667, lr: [7.976644307687256e-05] \n",
      "Episode 49, Epsilon: 0.14, Total Reward: 375.33999663218856, Steps: 6663, Max Reward: 29.639999861828983, lr: [7.17897987691853e-05] \n",
      "Episode 50, Epsilon: 0.13, Total Reward: 383.479996769689, Steps: 5891, Max Reward: 29.87999984715134, lr: [7.17897987691853e-05] \n",
      "Episode 51, Epsilon: 0.12, Total Reward: 383.4749967958778, Steps: 6058, Max Reward: 19.474999893456697, lr: [6.461081889226677e-05] \n",
      "Episode 52, Epsilon: 0.12, Total Reward: 386.8849968891591, Steps: 5801, Max Reward: 19.684999901801348, lr: [6.461081889226677e-05] \n",
      "Episode 53, Epsilon: 0.11, Total Reward: 373.2499964898452, Steps: 6458, Max Reward: 19.949999898672104, lr: [5.81497370030401e-05] \n",
      "Episode 54, Epsilon: 0.11, Total Reward: 384.6699968734756, Steps: 6064, Max Reward: 15.469999923370779, lr: [5.81497370030401e-05] \n",
      "Episode 55, Epsilon: 0.11, Total Reward: 391.8999970247969, Steps: 5545, Max Reward: 24.399999901652336, lr: [5.233476330273609e-05] \n",
      "Episode 56, Epsilon: 0.10, Total Reward: 319.17499667033553, Steps: 14188, Max Reward: 19.674999884329736, lr: [5.233476330273609e-05] \n",
      "Episode 57, Epsilon: 0.10, Total Reward: 392.1999970320612, Steps: 5438, Max Reward: 24.79999988526106, lr: [4.7101286972462485e-05] \n",
      "Episode 58, Epsilon: 0.10, Total Reward: 386.6849968805909, Steps: 5780, Max Reward: 24.98499987181276, lr: [4.7101286972462485e-05] \n",
      "Episode 59, Epsilon: 0.10, Total Reward: 391.2599970744923, Steps: 5806, Max Reward: 20.084999890998006, lr: [4.239115827521624e-05] \n",
      "Episode 60, Epsilon: 0.10, Total Reward: 383.23999696504325, Steps: 6545, Max Reward: 24.93999988399446, lr: [4.239115827521624e-05] \n",
      "Episode 61, Epsilon: 0.10, Total Reward: 388.694996913895, Steps: 5555, Max Reward: 25.19499988015741, lr: [3.8152042447694614e-05] \n",
      "Episode 62, Epsilon: 0.10, Total Reward: 391.71999709680676, Steps: 5686, Max Reward: 15.784999924711883, lr: [3.8152042447694614e-05] \n",
      "Episode 63, Epsilon: 0.10, Total Reward: 384.4899967601523, Steps: 5680, Max Reward: 20.189999889582396, lr: [3.433683820292515e-05] \n",
      "Episode 64, Epsilon: 0.10, Total Reward: 385.9149969005957, Steps: 6002, Max Reward: 20.174999906681478, lr: [3.433683820292515e-05] \n",
      "Episode 65, Epsilon: 0.10, Total Reward: 388.7899969443679, Steps: 5626, Max Reward: 29.38999987579882, lr: [3.090315438263264e-05] \n",
      "Episode 66, Epsilon: 0.10, Total Reward: 385.77999686822295, Steps: 5879, Max Reward: 20.379999903962016, lr: [3.090315438263264e-05] \n",
      "Episode 67, Epsilon: 0.10, Total Reward: 387.88999690581113, Steps: 5711, Max Reward: 20.389999898150563, lr: [2.7812838944369376e-05] \n",
      "Episode 68, Epsilon: 0.10, Total Reward: 380.45499677211046, Steps: 6349, Max Reward: 29.454999847337604, lr: [2.7812838944369376e-05] \n",
      "Episode 69, Epsilon: 0.10, Total Reward: 382.9449969260022, Steps: 6378, Max Reward: 19.844999911263585, lr: [2.503155504993244e-05] \n",
      "Episode 70, Epsilon: 0.10, Total Reward: 389.05999698024243, Steps: 5716, Max Reward: 24.584999877028167, lr: [2.503155504993244e-05] \n",
      "Episode 71, Epsilon: 0.10, Total Reward: 389.2899969657883, Steps: 5622, Max Reward: 25.289999873377383, lr: [2.2528399544939195e-05] \n",
      "Episode 72, Epsilon: 0.10, Total Reward: 387.67999694962054, Steps: 5868, Max Reward: 20.37999989837408, lr: [2.2528399544939195e-05] \n",
      "Episode 73, Epsilon: 0.10, Total Reward: 375.70499683264643, Steps: 7001, Max Reward: 24.404999874532223, lr: [2.0275559590445276e-05] \n",
      "Episode 74, Epsilon: 0.10, Total Reward: 390.9799970909953, Steps: 5873, Max Reward: 29.1799998562783, lr: [2.0275559590445276e-05] \n",
      "Episode 75, Epsilon: 0.10, Total Reward: 385.0849968120456, Steps: 5760, Max Reward: 20.18499989528209, lr: [1.8248003631400748e-05] \n",
      "Episode 76, Epsilon: 0.10, Total Reward: 391.38999705575407, Steps: 5660, Max Reward: 20.089999907650054, lr: [1.8248003631400748e-05] \n",
      "Episode 77, Epsilon: 0.10, Total Reward: 389.7899969872087, Steps: 5635, Max Reward: 19.689999890513718, lr: [1.6423203268260675e-05] \n",
      "Episode 78, Epsilon: 0.10, Total Reward: 389.3849969962612, Steps: 5749, Max Reward: 29.284999860450625, lr: [1.6423203268260675e-05] \n",
      "Episode 79, Epsilon: 0.10, Total Reward: 391.59499703813344, Steps: 5521, Max Reward: 19.794999883510172, lr: [1.4780882941434607e-05] \n",
      "Episode 80, Epsilon: 0.10, Total Reward: 388.5899969357997, Steps: 5603, Max Reward: 24.38999987952411, lr: [1.4780882941434607e-05] \n",
      "Episode 81, Epsilon: 0.10, Total Reward: 390.98999703861773, Steps: 5613, Max Reward: 19.989999886602163, lr: [1.3302794647291146e-05] \n",
      "Episode 82, Epsilon: 0.10, Total Reward: 388.47499700449407, Steps: 5895, Max Reward: 24.474999889731407, lr: [1.3302794647291146e-05] \n",
      "Episode 83, Epsilon: 0.10, Total Reward: 388.3699970319867, Steps: 5956, Max Reward: 24.36999987438321, lr: [1.1972515182562031e-05] \n",
      "Episode 84, Epsilon: 0.10, Total Reward: 386.344997077249, Steps: 6400, Max Reward: 24.04499986767769, lr: [1.1972515182562031e-05] \n",
      "Episode 85, Epsilon: 0.10, Total Reward: 387.67499697580934, Steps: 5903, Max Reward: 20.174999901093543, lr: [1.0775263664305828e-05] \n",
      "Episode 86, Epsilon: 0.10, Total Reward: 391.58999706432223, Steps: 5741, Max Reward: 23.989999862387776, lr: [1.0775263664305828e-05] \n",
      "Episode 87, Epsilon: 0.10, Total Reward: 387.3799969367683, Steps: 5812, Max Reward: 24.17999984882772, lr: [9.697737297875246e-06] \n",
      "Episode 88, Epsilon: 0.10, Total Reward: 386.534996971488, Steps: 6005, Max Reward: 24.664999892935157, lr: [9.697737297875246e-06] \n",
      "Episode 89, Epsilon: 0.10, Total Reward: 388.4849969577044, Steps: 5817, Max Reward: 29.184999850578606, lr: [8.727963568087722e-06] \n",
      "Episode 90, Epsilon: 0.10, Total Reward: 388.9999969005585, Steps: 5480, Max Reward: 24.29999987501651, lr: [8.727963568087722e-06] \n",
      "Episode 91, Epsilon: 0.10, Total Reward: 391.2999969990924, Steps: 5411, Max Reward: 24.399999890476465, lr: [7.85516721127895e-06] \n",
      "Episode 92, Epsilon: 0.10, Total Reward: 387.3949968582019, Steps: 5571, Max Reward: 19.694999895989895, lr: [7.85516721127895e-06] \n",
      "Episode 93, Epsilon: 0.10, Total Reward: 387.4899968886748, Steps: 5639, Max Reward: 29.489999857731164, lr: [7.069650490151056e-06] \n",
      "Episode 94, Epsilon: 0.10, Total Reward: 390.28999700862914, Steps: 5697, Max Reward: 30.289999864064157, lr: [7.069650490151056e-06] \n",
      "Episode 95, Epsilon: 0.10, Total Reward: 384.37499683443457, Steps: 5888, Max Reward: 25.074999881908298, lr: [6.362685441135951e-06] \n",
      "Episode 96, Epsilon: 0.10, Total Reward: 388.39499690104276, Steps: 5584, Max Reward: 24.8949998896569, lr: [6.362685441135951e-06] \n",
      "Episode 97, Epsilon: 0.10, Total Reward: 387.9899969100952, Steps: 5625, Max Reward: 34.289999856613576, lr: [5.7264168970223554e-06] \n",
      "Episode 98, Epsilon: 0.10, Total Reward: 316.52499577589333, Steps: 11601, Max Reward: 18.724999900907278, lr: [5.7264168970223554e-06] \n",
      "Episode 99, Epsilon: 0.10, Total Reward: 392.5999970547855, Steps: 5399, Max Reward: 24.499999889172614, lr: [5.15377520732012e-06] \n"
     ]
    }
   ],
   "source": [
    "env = battle_v4.env(map_size=45, render_mode=None)\n",
    "\n",
    "trainer = Trainer(env, config_qnet=None, input_shape=env.observation_space(\"red_0\").shape, action_shape=env.action_space(\"red_0\").n)\n",
    "trainer.training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1086b98",
   "metadata": {
    "papermill": {
     "duration": 0.012804,
     "end_time": "2024-12-16T17:14:38.352754",
     "exception": false,
     "start_time": "2024-12-16T17:14:38.339950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lưu mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4dceb69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T17:14:38.373739Z",
     "iopub.status.busy": "2024-12-16T17:14:38.373320Z",
     "iopub.status.idle": "2024-12-16T17:14:38.384090Z",
     "shell.execute_reply": "2024-12-16T17:14:38.383128Z"
    },
    "papermill": {
     "duration": 0.02252,
     "end_time": "2024-12-16T17:14:38.386035",
     "exception": false,
     "start_time": "2024-12-16T17:14:38.363515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(trainer.q_network.state_dict(), \"models/blue_resnet_vs_random.pt\")\n",
    "print(\"Training complete. Model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "manhtms1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 798.850498,
   "end_time": "2024-12-16T17:14:39.920433",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-16T17:01:21.069935",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
